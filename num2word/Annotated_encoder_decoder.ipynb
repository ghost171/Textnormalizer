{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CUDA: True\n",
      "cuda:0\n"
     ]
    }
   ],
   "source": [
    "%matplotlib inline\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import math, copy, time\n",
    "import matplotlib.pyplot as plt\n",
    "from torch.nn.utils.rnn import pack_padded_sequence, pad_packed_sequence\n",
    "from IPython.core.debugger import set_trace\n",
    "\n",
    "# we will use CUDA if it is available\n",
    "USE_CUDA = torch.cuda.is_available()\n",
    "DEVICE=torch.device('cuda:0') # or set to 'cpu'\n",
    "print(\"CUDA:\", USE_CUDA)\n",
    "print(DEVICE)\n",
    "\n",
    "seed = 42\n",
    "np.random.seed(seed)\n",
    "torch.manual_seed(seed)\n",
    "torch.cuda.manual_seed(seed)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "class EncoderDecoder(nn.Module):\n",
    "    \"\"\"\n",
    "    A standard Encoder-Decoder architecture. Base for this and many \n",
    "    other models.\n",
    "    \"\"\"\n",
    "    def __init__(self, encoder, decoder, src_embed, trg_embed, generator):\n",
    "        super(EncoderDecoder, self).__init__()\n",
    "        self.encoder = encoder\n",
    "        self.decoder = decoder\n",
    "        self.src_embed = src_embed\n",
    "        self.trg_embed = trg_embed\n",
    "        self.generator = generator\n",
    "        \n",
    "    def forward(self, src, trg, src_mask, trg_mask, src_lengths, trg_lengths):\n",
    "        \"\"\"Take in and process masked src and target sequences.\"\"\"\n",
    "        encoder_hidden, encoder_final = self.encode(src, src_mask, src_lengths)\n",
    "        return self.decode(encoder_hidden, encoder_final, src_mask, trg, trg_mask)\n",
    "    \n",
    "    def encode(self, src, src_mask, src_lengths):\n",
    "        return self.encoder(self.src_embed(src), src_mask, src_lengths)\n",
    "    \n",
    "    def decode(self, encoder_hidden, encoder_final, src_mask, trg, trg_mask,\n",
    "               decoder_hidden=None):\n",
    "        return self.decoder(self.trg_embed(trg), encoder_hidden, encoder_final,\n",
    "                            src_mask, trg_mask, hidden=decoder_hidden)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Generator(nn.Module):\n",
    "    \"\"\"Define standard linear + softmax generation step.\"\"\"\n",
    "    def __init__(self, hidden_size, vocab_size):\n",
    "        super(Generator, self).__init__()\n",
    "        self.proj = nn.Linear(hidden_size, vocab_size, bias=False)\n",
    "\n",
    "    def forward(self, x):\n",
    "        return F.log_softmax(self.proj(x), dim=-1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Encoder(nn.Module):\n",
    "    \"\"\"Encodes a sequence of word embeddings\"\"\"\n",
    "    def __init__(self, input_size, hidden_size, num_layers=1, dropout=0.):\n",
    "        super(Encoder, self).__init__()\n",
    "        self.num_layers = num_layers\n",
    "        self.rnn = nn.GRU(input_size, hidden_size, num_layers, \n",
    "                          batch_first=True, bidirectional=True, dropout=dropout)\n",
    "        \n",
    "    def forward(self, x, mask, lengths):\n",
    "        \"\"\"\n",
    "        Applies a bidirectional GRU to sequence of embeddings x.\n",
    "        The input mini-batch x needs to be sorted by length.\n",
    "        x should have dimensions [batch, time, dim].\n",
    "        \"\"\"\n",
    "        packed = pack_padded_sequence(x, lengths, batch_first=True)\n",
    "        output, final = self.rnn(packed)\n",
    "        output, _ = pad_packed_sequence(output, batch_first=True)\n",
    "\n",
    "        # we need to manually concatenate the final states for both directions\n",
    "        fwd_final = final[0:final.size(0):2]\n",
    "        bwd_final = final[1:final.size(0):2]\n",
    "        final = torch.cat([fwd_final, bwd_final], dim=2)  # [num_layers, batch, 2*dim]\n",
    "\n",
    "        return output, final\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Decoder(nn.Module):\n",
    "    \"\"\"A conditional RNN decoder with attention.\"\"\"\n",
    "    \n",
    "    def __init__(self, emb_size, hidden_size, attention, num_layers=1, dropout=0.5,\n",
    "                 bridge=True):\n",
    "        super(Decoder, self).__init__()\n",
    "        \n",
    "        self.hidden_size = hidden_size\n",
    "        self.num_layers = num_layers\n",
    "        self.attention = attention\n",
    "        self.dropout = dropout\n",
    "                 \n",
    "        self.rnn = nn.GRU(emb_size + 2 * hidden_size, hidden_size, num_layers,\n",
    "                          batch_first=True, dropout=dropout)\n",
    "                 \n",
    "        # to initialize from the final encoder state\n",
    "        self.bridge = nn.Linear(2 * hidden_size, hidden_size, bias=True) if bridge else None\n",
    "\n",
    "        self.dropout_layer = nn.Dropout(p=dropout)\n",
    "        self.pre_output_layer = nn.Linear(hidden_size + 2 * hidden_size + emb_size,\n",
    "                                          hidden_size, bias=False)\n",
    "        \n",
    "    def forward_step(self, prev_embed, encoder_hidden, src_mask, proj_key, hidden):\n",
    "        \"\"\"Perform a single decoder step (1 word)\"\"\"\n",
    "\n",
    "        # compute context vector using attention mechanism\n",
    "        query = hidden[-1].unsqueeze(1)  # [#layers, B, D] -> [B, 1, D]\n",
    "        context, attn_probs = self.attention(\n",
    "            query=query, proj_key=proj_key,\n",
    "            value=encoder_hidden, mask=src_mask)\n",
    "\n",
    "        # update rnn hidden state\n",
    "        rnn_input = torch.cat([prev_embed, context], dim=2)\n",
    "        output, hidden = self.rnn(rnn_input, hidden)\n",
    "        \n",
    "        pre_output = torch.cat([prev_embed, output, context], dim=2)\n",
    "        pre_output = self.dropout_layer(pre_output)\n",
    "        pre_output = self.pre_output_layer(pre_output)\n",
    "\n",
    "        return output, hidden, pre_output\n",
    "    \n",
    "    def forward(self, trg_embed, encoder_hidden, encoder_final, \n",
    "                src_mask, trg_mask, hidden=None, max_len=None):\n",
    "        \"\"\"Unroll the decoder one step at a time.\"\"\"\n",
    "                                         \n",
    "        # the maximum number of steps to unroll the RNN\n",
    "        if max_len is None:\n",
    "            max_len = trg_mask.size(-1)\n",
    "\n",
    "        # initialize decoder hidden state\n",
    "        if hidden is None:\n",
    "            hidden = self.init_hidden(encoder_final)\n",
    "        \n",
    "        # pre-compute projected encoder hidden states\n",
    "        # (the \"keys\" for the attention mechanism)\n",
    "        # this is only done for efficiency\n",
    "        proj_key = self.attention.key_layer(encoder_hidden)\n",
    "        \n",
    "        # here we store all intermediate hidden states and pre-output vectors\n",
    "        decoder_states = []\n",
    "        pre_output_vectors = []\n",
    "        \n",
    "        # unroll the decoder RNN for max_len steps\n",
    "        for i in range(max_len):\n",
    "            prev_embed = trg_embed[:, i].unsqueeze(1)\n",
    "            output, hidden, pre_output = self.forward_step(\n",
    "              prev_embed, encoder_hidden, src_mask, proj_key, hidden)\n",
    "            decoder_states.append(output)\n",
    "            pre_output_vectors.append(pre_output)\n",
    "\n",
    "        decoder_states = torch.cat(decoder_states, dim=1)\n",
    "        pre_output_vectors = torch.cat(pre_output_vectors, dim=1)\n",
    "        return decoder_states, hidden, pre_output_vectors  # [B, N, D]\n",
    "\n",
    "    def init_hidden(self, encoder_final):\n",
    "        \"\"\"Returns the initial decoder state,\n",
    "        conditioned on the final encoder state.\"\"\"\n",
    "\n",
    "        if encoder_final is None:\n",
    "            return None  # start with zeros\n",
    "\n",
    "        return torch.tanh(self.bridge(encoder_final))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "class BahdanauAttention(nn.Module):\n",
    "    \"\"\"Implements Bahdanau (MLP) attention\"\"\"\n",
    "    \n",
    "    def __init__(self, hidden_size, key_size=None, query_size=None):\n",
    "        super(BahdanauAttention, self).__init__()\n",
    "        \n",
    "        # We assume a bi-directional encoder so key_size is 2*hidden_size\n",
    "        key_size = 2 * hidden_size if key_size is None else key_size\n",
    "        query_size = hidden_size if query_size is None else query_size\n",
    "\n",
    "        self.key_layer = nn.Linear(key_size, hidden_size, bias=False)\n",
    "        self.query_layer = nn.Linear(query_size, hidden_size, bias=False)\n",
    "        self.energy_layer = nn.Linear(hidden_size, 1, bias=False)\n",
    "        \n",
    "        # to store attention scores\n",
    "        self.alphas = None\n",
    "        \n",
    "    def forward(self, query=None, proj_key=None, value=None, mask=None):\n",
    "        assert mask is not None, \"mask is required\"\n",
    "\n",
    "        # We first project the query (the decoder state).\n",
    "        # The projected keys (the encoder states) were already pre-computated.\n",
    "        query = self.query_layer(query)\n",
    "        \n",
    "        # Calculate scores.\n",
    "        scores = self.energy_layer(torch.tanh(query + proj_key))\n",
    "        scores = scores.squeeze(2).unsqueeze(1)\n",
    "        \n",
    "        # Mask out invalid positions.\n",
    "        # The mask marks valid positions so we invert it using `mask & 0`.\n",
    "        scores.data.masked_fill_(mask == 0, -float('inf'))\n",
    "        \n",
    "        # Turn scores to probabilities.\n",
    "        alphas = F.softmax(scores, dim=-1)\n",
    "        self.alphas = alphas        \n",
    "        \n",
    "        # The context vector is the weighted sum of the values.\n",
    "        context = torch.bmm(alphas, value)\n",
    "        \n",
    "        # context shape: [B, 1, 2D], alphas shape: [B, 1, M]\n",
    "        return context, alphas\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_model(src_vocab, tgt_vocab, emb_size=256, hidden_size=512, num_layers=1, dropout=0.1):\n",
    "    \"Helper: Construct a model from hyperparameters.\"\n",
    "\n",
    "    attention = BahdanauAttention(hidden_size)\n",
    "\n",
    "    model = EncoderDecoder(\n",
    "        Encoder(emb_size, hidden_size, num_layers=num_layers, dropout=dropout),\n",
    "        Decoder(emb_size, hidden_size, attention, num_layers=num_layers, dropout=dropout),\n",
    "        nn.Embedding(src_vocab, emb_size),\n",
    "        nn.Embedding(tgt_vocab, emb_size),\n",
    "        Generator(hidden_size, tgt_vocab))\n",
    "\n",
    "    return model.cuda() if USE_CUDA else model\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Batch:\n",
    "    \"\"\"Object for holding a batch of data with mask during training.\n",
    "    Input is a batch from a torch text iterator.\n",
    "    \"\"\"\n",
    "    def __init__(self, src, trg, pad_index=0):\n",
    "        \n",
    "        src, src_lengths = src\n",
    "        \n",
    "        self.src = src\n",
    "        self.src_lengths = src_lengths\n",
    "        self.src_mask = (src != pad_index).unsqueeze(-2)\n",
    "        self.nseqs = src.size(0)\n",
    "        \n",
    "        self.trg = None\n",
    "        self.trg_y = None\n",
    "        self.trg_mask = None\n",
    "        self.trg_lengths = None\n",
    "        self.ntokens = None\n",
    "\n",
    "        if trg is not None:\n",
    "            trg, trg_lengths = trg\n",
    "            self.trg = trg[:, :-1]\n",
    "            self.trg_lengths = trg_lengths\n",
    "            self.trg_y = trg[:, 1:]\n",
    "            self.trg_mask = (self.trg_y != pad_index)\n",
    "            self.ntokens = (self.trg_y != pad_index).data.sum().item()\n",
    "        \n",
    "        if USE_CUDA:\n",
    "            self.src = self.src.cuda()\n",
    "            self.src_mask = self.src_mask.cuda()\n",
    "\n",
    "            if trg is not None:\n",
    "                self.trg = self.trg.cuda()\n",
    "                self.trg_y = self.trg_y.cuda()\n",
    "                self.trg_mask = self.trg_mask.cuda()\n",
    "                \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_epoch(data_iter, model, loss_compute, print_every=50):\n",
    "    \"\"\"Standard Training and Logging Function\"\"\"\n",
    "\n",
    "    start = time.time()\n",
    "    total_tokens = 0\n",
    "    total_loss = 0\n",
    "    print_tokens = 0\n",
    "\n",
    "    for i, batch in enumerate(data_iter, 1):\n",
    "        \n",
    "        out, _, pre_output = model.forward(batch.src, batch.trg,\n",
    "                                           batch.src_mask, batch.trg_mask,\n",
    "                                           batch.src_lengths, batch.trg_lengths)\n",
    "        loss = loss_compute(pre_output, batch.trg_y, batch.nseqs)\n",
    "        total_loss += loss\n",
    "        total_tokens += batch.ntokens\n",
    "        print_tokens += batch.ntokens\n",
    "        \n",
    "        if model.training and i % print_every == 0:\n",
    "            elapsed = time.time() - start\n",
    "            print(\"Epoch Step: %d Loss: %f Tokens per Sec: %f\" %\n",
    "                    (i, loss / batch.nseqs, print_tokens / elapsed))\n",
    "            start = time.time()\n",
    "            print_tokens = 0\n",
    "\n",
    "    return math.exp(total_loss / float(total_tokens))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def data_gen(num_words=11, batch_size=16, num_batches=100, length=10, pad_index=0, sos_index=1):\n",
    "    \"\"\"Generate random data for a src-tgt copy task.\"\"\"\n",
    "    for i in range(num_batches):\n",
    "        data = torch.from_numpy(\n",
    "          np.random.randint(1, num_words, size=(batch_size, length)))\n",
    "        data[:, 0] = sos_index\n",
    "        data = data.cuda() if USE_CUDA else data\n",
    "        src = data[:, 1:]\n",
    "        trg = data\n",
    "        src_lengths = [length-1] * batch_size\n",
    "        trg_lengths = [length] * batch_size\n",
    "        yield Batch((src, src_lengths), (trg, trg_lengths), pad_index=pad_index)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SimpleLossCompute:\n",
    "    \"\"\"A simple loss compute and train function.\"\"\"\n",
    "\n",
    "    def __init__(self, generator, criterion, opt=None):\n",
    "        self.generator = generator\n",
    "        self.criterion = criterion\n",
    "        self.opt = opt\n",
    "\n",
    "    def __call__(self, x, y, norm):\n",
    "        x = self.generator(x)\n",
    "        loss = self.criterion(x.contiguous().view(-1, x.size(-1)),\n",
    "                              y.contiguous().view(-1))\n",
    "        loss = loss / norm\n",
    "\n",
    "        if self.opt is not None:\n",
    "            loss.backward()          \n",
    "            self.opt.step()\n",
    "            self.opt.zero_grad()\n",
    "\n",
    "        return loss.data.item() * norm\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def greedy_decode(model, src, src_mask, src_lengths, max_len=100, sos_index=1, eos_index=None):\n",
    "    \"\"\"Greedily decode a sentence.\"\"\"\n",
    "\n",
    "    with torch.no_grad():\n",
    "        encoder_hidden, encoder_final = model.encode(src, src_mask, src_lengths)\n",
    "        prev_y = torch.ones(1, 1).fill_(sos_index).type_as(src)\n",
    "        trg_mask = torch.ones_like(prev_y)\n",
    "\n",
    "    output = []\n",
    "    attention_scores = []\n",
    "    hidden = None\n",
    "\n",
    "    for i in range(max_len):\n",
    "        with torch.no_grad():\n",
    "            out, hidden, pre_output = model.decode(\n",
    "              encoder_hidden, encoder_final, src_mask,\n",
    "              prev_y, trg_mask, hidden)\n",
    "\n",
    "            # we predict from the pre-output layer, which is\n",
    "            # a combination of Decoder state, prev emb, and context\n",
    "            prob = model.generator(pre_output[:, -1])\n",
    "\n",
    "        _, next_word = torch.max(prob, dim=1)\n",
    "        next_word = next_word.data.item()\n",
    "        output.append(next_word)\n",
    "        prev_y = torch.ones(1, 1).type_as(src).fill_(next_word)\n",
    "        attention_scores.append(model.decoder.attention.alphas.cpu().numpy())\n",
    "    \n",
    "    output = np.array(output)\n",
    "        \n",
    "    # cut off everything starting from </s> \n",
    "    # (only when eos_index provided)\n",
    "    if eos_index is not None:\n",
    "        first_eos = np.where(output==eos_index)[0]\n",
    "        if len(first_eos) > 0:\n",
    "            output = output[:first_eos[0]]      \n",
    "    \n",
    "    return output, np.concatenate(attention_scores, axis=1)\n",
    "  \n",
    "\n",
    "def lookup_words(x, vocab=None):\n",
    "    if vocab is not None:\n",
    "        x = [vocab.itos[i] for i in x]\n",
    "\n",
    "    return [str(t) for t in x]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def print_examples(example_iter, model, n=2, max_len=100, \n",
    "                   sos_index=1, \n",
    "                   src_eos_index=None, \n",
    "                   trg_eos_index=None, \n",
    "                   src_vocab=None, trg_vocab=None):\n",
    "    \"\"\"Prints N examples. Assumes batch size of 1.\"\"\"\n",
    "\n",
    "    model.eval()\n",
    "    count = 0\n",
    "    print()\n",
    "    \n",
    "    if src_vocab is not None and trg_vocab is not None:\n",
    "        src_eos_index = src_vocab.stoi[EOS_TOKEN]\n",
    "        trg_sos_index = trg_vocab.stoi[SOS_TOKEN]\n",
    "        trg_eos_index = trg_vocab.stoi[EOS_TOKEN]\n",
    "    else:\n",
    "        src_eos_index = None\n",
    "        trg_sos_index = 1\n",
    "        trg_eos_index = None\n",
    "        \n",
    "    for i, batch in enumerate(example_iter):\n",
    "      \n",
    "        src = batch.src.cpu().numpy()[0, :]\n",
    "        trg = batch.trg_y.cpu().numpy()[0, :]\n",
    "\n",
    "        # remove </s> (if it is there)\n",
    "        src = src[:-1] if src[-1] == src_eos_index else src\n",
    "        trg = trg[:-1] if trg[-1] == trg_eos_index else trg      \n",
    "      \n",
    "        result, _ = greedy_decode(\n",
    "          model, batch.src, batch.src_mask, batch.src_lengths,\n",
    "          max_len=max_len, sos_index=trg_sos_index, eos_index=trg_eos_index)\n",
    "        print(\"Example #%d\" % (i+1))\n",
    "        print(\"Src : \", \" \".join(lookup_words(src, vocab=src_vocab)))\n",
    "        print(\"Trg : \", \" \".join(lookup_words(trg, vocab=trg_vocab)))\n",
    "        print(\"Pred: \", \" \".join(lookup_words(result, vocab=trg_vocab)))\n",
    "        print()\n",
    "        \n",
    "        count += 1\n",
    "        if count == n:\n",
    "            break\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_copy_task():\n",
    "    \"\"\"Train the simple copy task.\"\"\"\n",
    "    num_words = 11\n",
    "    criterion = nn.NLLLoss(reduction=\"sum\", ignore_index=0)\n",
    "    model = make_model(num_words, num_words, emb_size=32, hidden_size=64)\n",
    "    optim = torch.optim.Adam(model.parameters(), lr=0.0003)\n",
    "    eval_data = list(data_gen(num_words=num_words, batch_size=1, num_batches=100))\n",
    " \n",
    "    dev_perplexities = []\n",
    "    \n",
    "    if USE_CUDA:\n",
    "        model.cuda()\n",
    "\n",
    "    for epoch in range(10):\n",
    "        \n",
    "        print(\"Epoch %d\" % epoch)\n",
    "\n",
    "        # train\n",
    "        model.train()\n",
    "        data = data_gen(num_words=num_words, batch_size=32, num_batches=100)\n",
    "        run_epoch(data, model,\n",
    "                  SimpleLossCompute(model.generator, criterion, optim))\n",
    "\n",
    "        # evaluate\n",
    "        model.eval()\n",
    "        with torch.no_grad(): \n",
    "            perplexity = run_epoch(eval_data, model,\n",
    "                                   SimpleLossCompute(model.generator, criterion, None))\n",
    "            print(\"Evaluation perplexity: %f\" % perplexity)\n",
    "            dev_perplexities.append(perplexity)\n",
    "            print_examples(eval_data, model, n=2, max_len=9)\n",
    "        \n",
    "    return dev_perplexities\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ghost/anaconda3/lib/python3.8/site-packages/torch/nn/modules/rnn.py:62: UserWarning: dropout option adds dropout after all but last recurrent layer, so non-zero dropout expects num_layers greater than 1, but got dropout=0.1 and num_layers=1\n",
      "  warnings.warn(\"dropout option adds dropout after all but last \"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0\n",
      "Epoch Step: 50 Loss: 19.720032 Tokens per Sec: 12029.871250\n",
      "Epoch Step: 100 Loss: 17.850552 Tokens per Sec: 22970.836178\n",
      "Evaluation perplexity: 7.165516\n",
      "\n",
      "Example #1\n",
      "Src :  4 8 5 7 10 3 7 8 5\n",
      "Trg :  4 8 5 7 10 3 7 8 5\n",
      "Pred:  5 8 7 5 8 7 5 8 7\n",
      "\n",
      "Example #2\n",
      "Src :  8 8 3 6 5 2 8 6 2\n",
      "Trg :  8 8 3 6 5 2 8 6 2\n",
      "Pred:  8 8 8 8 8 8 8 8 8\n",
      "\n",
      "Epoch 1\n",
      "Epoch Step: 50 Loss: 15.429652 Tokens per Sec: 23875.332438\n",
      "Epoch Step: 100 Loss: 11.758204 Tokens per Sec: 22620.558732\n",
      "Evaluation perplexity: 3.761093\n",
      "\n",
      "Example #1\n",
      "Src :  4 8 5 7 10 3 7 8 5\n",
      "Trg :  4 8 5 7 10 3 7 8 5\n",
      "Pred:  4 8 5 7 5 3 8 7 5\n",
      "\n",
      "Example #2\n",
      "Src :  8 8 3 6 5 2 8 6 2\n",
      "Trg :  8 8 3 6 5 2 8 6 2\n",
      "Pred:  8 8 3 8 2 5 8 3 2\n",
      "\n",
      "Epoch 2\n",
      "Epoch Step: 50 Loss: 9.917424 Tokens per Sec: 22669.794603\n",
      "Epoch Step: 100 Loss: 8.949947 Tokens per Sec: 22135.606976\n",
      "Evaluation perplexity: 2.573576\n",
      "\n",
      "Example #1\n",
      "Src :  4 8 5 7 10 3 7 8 5\n",
      "Trg :  4 8 5 7 10 3 7 8 5\n",
      "Pred:  4 8 5 7 3 5 7 8 10\n",
      "\n",
      "Example #2\n",
      "Src :  8 8 3 6 5 2 8 6 2\n",
      "Trg :  8 8 3 6 5 2 8 6 2\n",
      "Pred:  8 8 3 8 5 2 6 8 2\n",
      "\n",
      "Epoch 3\n",
      "Epoch Step: 50 Loss: 7.275529 Tokens per Sec: 22971.954488\n",
      "Epoch Step: 100 Loss: 6.582399 Tokens per Sec: 22665.574980\n",
      "Evaluation perplexity: 2.072119\n",
      "\n",
      "Example #1\n",
      "Src :  4 8 5 7 10 3 7 8 5\n",
      "Trg :  4 8 5 7 10 3 7 8 5\n",
      "Pred:  4 8 5 7 3 5 8 7 5\n",
      "\n",
      "Example #2\n",
      "Src :  8 8 3 6 5 2 8 6 2\n",
      "Trg :  8 8 3 6 5 2 8 6 2\n",
      "Pred:  8 8 3 6 2 5 8 2 6\n",
      "\n",
      "Epoch 4\n",
      "Epoch Step: 50 Loss: 5.942049 Tokens per Sec: 22608.433378\n",
      "Epoch Step: 100 Loss: 4.906624 Tokens per Sec: 20951.739521\n",
      "Evaluation perplexity: 1.765160\n",
      "\n",
      "Example #1\n",
      "Src :  4 8 5 7 10 3 7 8 5\n",
      "Trg :  4 8 5 7 10 3 7 8 5\n",
      "Pred:  4 8 5 7 3 5 10 8 7\n",
      "\n",
      "Example #2\n",
      "Src :  8 8 3 6 5 2 8 6 2\n",
      "Trg :  8 8 3 6 5 2 8 6 2\n",
      "Pred:  8 8 3 6 5 2 8 6 2\n",
      "\n",
      "Epoch 5\n",
      "Epoch Step: 50 Loss: 5.266299 Tokens per Sec: 18346.805605\n",
      "Epoch Step: 100 Loss: 4.140486 Tokens per Sec: 22977.906080\n",
      "Evaluation perplexity: 1.565404\n",
      "\n",
      "Example #1\n",
      "Src :  4 8 5 7 10 3 7 8 5\n",
      "Trg :  4 8 5 7 10 3 7 8 5\n",
      "Pred:  4 8 5 7 3 10 5 7 8\n",
      "\n",
      "Example #2\n",
      "Src :  8 8 3 6 5 2 8 6 2\n",
      "Trg :  8 8 3 6 5 2 8 6 2\n",
      "Pred:  8 8 3 6 5 2 8 6 2\n",
      "\n",
      "Epoch 6\n",
      "Epoch Step: 50 Loss: 3.958579 Tokens per Sec: 19923.758591\n",
      "Epoch Step: 100 Loss: 3.681249 Tokens per Sec: 21151.017226\n",
      "Evaluation perplexity: 1.455613\n",
      "\n",
      "Example #1\n",
      "Src :  4 8 5 7 10 3 7 8 5\n",
      "Trg :  4 8 5 7 10 3 7 8 5\n",
      "Pred:  4 8 5 7 10 3 5 8 7\n",
      "\n",
      "Example #2\n",
      "Src :  8 8 3 6 5 2 8 6 2\n",
      "Trg :  8 8 3 6 5 2 8 6 2\n",
      "Pred:  8 8 3 6 5 2 8 8 6\n",
      "\n",
      "Epoch 7\n",
      "Epoch Step: 50 Loss: 2.987803 Tokens per Sec: 22471.768003\n",
      "Epoch Step: 100 Loss: 2.790763 Tokens per Sec: 22564.073505\n",
      "Evaluation perplexity: 1.366847\n",
      "\n",
      "Example #1\n",
      "Src :  4 8 5 7 10 3 7 8 5\n",
      "Trg :  4 8 5 7 10 3 7 8 5\n",
      "Pred:  4 8 5 7 10 3 7 8 5\n",
      "\n",
      "Example #2\n",
      "Src :  8 8 3 6 5 2 8 6 2\n",
      "Trg :  8 8 3 6 5 2 8 6 2\n",
      "Pred:  8 8 3 6 5 2 8 6 2\n",
      "\n",
      "Epoch 8\n",
      "Epoch Step: 50 Loss: 2.148365 Tokens per Sec: 18617.562004\n",
      "Epoch Step: 100 Loss: 2.303623 Tokens per Sec: 20311.993793\n",
      "Evaluation perplexity: 1.275108\n",
      "\n",
      "Example #1\n",
      "Src :  4 8 5 7 10 3 7 8 5\n",
      "Trg :  4 8 5 7 10 3 7 8 5\n",
      "Pred:  4 8 5 7 10 3 7 8 5\n",
      "\n",
      "Example #2\n",
      "Src :  8 8 3 6 5 2 8 6 2\n",
      "Trg :  8 8 3 6 5 2 8 6 2\n",
      "Pred:  8 8 3 6 5 2 8 6 2\n",
      "\n",
      "Epoch 9\n",
      "Epoch Step: 50 Loss: 2.088934 Tokens per Sec: 22406.010354\n",
      "Epoch Step: 100 Loss: 2.050290 Tokens per Sec: 22405.411904\n",
      "Evaluation perplexity: 1.195517\n",
      "\n",
      "Example #1\n",
      "Src :  4 8 5 7 10 3 7 8 5\n",
      "Trg :  4 8 5 7 10 3 7 8 5\n",
      "Pred:  4 8 5 7 10 3 7 8 5\n",
      "\n",
      "Example #2\n",
      "Src :  8 8 3 6 5 2 8 6 2\n",
      "Trg :  8 8 3 6 5 2 8 6 2\n",
      "Pred:  8 8 3 6 5 2 8 6 2\n",
      "\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": "<Figure size 432x288 with 1 Axes>"
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "dev_perplexities = train_copy_task()\n",
    "\n",
    "def plot_perplexity(perplexities):\n",
    "    \"\"\"plot perplexities\"\"\"\n",
    "    plt.title(\"Perplexity per Epoch\")\n",
    "    plt.xlabel(\"Epoch\")\n",
    "    plt.ylabel(\"Perplexity\")\n",
    "    plt.plot(perplexities)\n",
    "    \n",
    "plot_perplexity(dev_perplexities)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A REAL WORLD EXAMPLE "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting git+git://github.com/pytorch/text\n",
      "  Cloning git://github.com/pytorch/text to /tmp/pip-req-build-ww3ulvpq\n",
      "  Running command git clone -q git://github.com/pytorch/text /tmp/pip-req-build-ww3ulvpq\n",
      "  Running command git submodule update --init --recursive -q\n",
      "Requirement already satisfied: spacy in /home/ghost/anaconda3/lib/python3.8/site-packages (3.1.0)\n",
      "Requirement already satisfied: tqdm in /home/ghost/anaconda3/lib/python3.8/site-packages (from torchtext==0.11.0a0+05cb992) (4.50.2)\n",
      "Requirement already satisfied: requests in /home/ghost/anaconda3/lib/python3.8/site-packages (from torchtext==0.11.0a0+05cb992) (2.24.0)\n",
      "Requirement already satisfied: torch in /home/ghost/anaconda3/lib/python3.8/site-packages (from torchtext==0.11.0a0+05cb992) (1.9.0)\n",
      "Requirement already satisfied: numpy in /home/ghost/anaconda3/lib/python3.8/site-packages (from torchtext==0.11.0a0+05cb992) (1.21.0)\n",
      "Requirement already satisfied: typer<0.4.0,>=0.3.0 in /home/ghost/anaconda3/lib/python3.8/site-packages (from spacy) (0.3.2)\n",
      "Requirement already satisfied: pydantic!=1.8,!=1.8.1,<1.9.0,>=1.7.4 in /home/ghost/anaconda3/lib/python3.8/site-packages (from spacy) (1.8.2)\n",
      "Requirement already satisfied: pathy>=0.3.5 in /home/ghost/anaconda3/lib/python3.8/site-packages (from spacy) (0.6.0)\n",
      "Requirement already satisfied: jinja2 in /home/ghost/anaconda3/lib/python3.8/site-packages (from spacy) (2.11.2)\n",
      "Requirement already satisfied: blis<0.8.0,>=0.4.0 in /home/ghost/anaconda3/lib/python3.8/site-packages (from spacy) (0.7.4)\n",
      "Requirement already satisfied: cymem<2.1.0,>=2.0.2 in /home/ghost/anaconda3/lib/python3.8/site-packages (from spacy) (2.0.5)\n",
      "Requirement already satisfied: spacy-legacy<3.1.0,>=3.0.7 in /home/ghost/anaconda3/lib/python3.8/site-packages (from spacy) (3.0.8)\n",
      "Requirement already satisfied: wasabi<1.1.0,>=0.8.1 in /home/ghost/anaconda3/lib/python3.8/site-packages (from spacy) (0.8.2)\n",
      "Requirement already satisfied: srsly<3.0.0,>=2.4.1 in /home/ghost/anaconda3/lib/python3.8/site-packages (from spacy) (2.4.1)\n",
      "Requirement already satisfied: preshed<3.1.0,>=3.0.2 in /home/ghost/anaconda3/lib/python3.8/site-packages (from spacy) (3.0.5)\n",
      "Requirement already satisfied: thinc<8.1.0,>=8.0.7 in /home/ghost/anaconda3/lib/python3.8/site-packages (from spacy) (8.0.7)\n",
      "Requirement already satisfied: setuptools in /home/ghost/anaconda3/lib/python3.8/site-packages (from spacy) (57.1.0)\n",
      "Requirement already satisfied: packaging>=20.0 in /home/ghost/anaconda3/lib/python3.8/site-packages (from spacy) (20.4)\n",
      "Requirement already satisfied: murmurhash<1.1.0,>=0.28.0 in /home/ghost/anaconda3/lib/python3.8/site-packages (from spacy) (1.0.5)\n",
      "Requirement already satisfied: catalogue<2.1.0,>=2.0.4 in /home/ghost/anaconda3/lib/python3.8/site-packages (from spacy) (2.0.4)\n",
      "Requirement already satisfied: pyparsing>=2.0.2 in /home/ghost/anaconda3/lib/python3.8/site-packages (from packaging>=20.0->spacy) (2.4.7)\n",
      "Requirement already satisfied: six in /home/ghost/anaconda3/lib/python3.8/site-packages (from packaging>=20.0->spacy) (1.15.0)\n",
      "Requirement already satisfied: smart-open<6.0.0,>=5.0.0 in /home/ghost/anaconda3/lib/python3.8/site-packages (from pathy>=0.3.5->spacy) (5.1.0)\n",
      "Requirement already satisfied: typing-extensions>=3.7.4.3 in /home/ghost/anaconda3/lib/python3.8/site-packages (from pydantic!=1.8,!=1.8.1,<1.9.0,>=1.7.4->spacy) (3.7.4.3)\n",
      "Requirement already satisfied: chardet<4,>=3.0.2 in /home/ghost/anaconda3/lib/python3.8/site-packages (from requests->torchtext==0.11.0a0+05cb992) (3.0.4)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /home/ghost/anaconda3/lib/python3.8/site-packages (from requests->torchtext==0.11.0a0+05cb992) (2020.6.20)\n",
      "Requirement already satisfied: idna<3,>=2.5 in /home/ghost/anaconda3/lib/python3.8/site-packages (from requests->torchtext==0.11.0a0+05cb992) (2.10)\n",
      "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /home/ghost/anaconda3/lib/python3.8/site-packages (from requests->torchtext==0.11.0a0+05cb992) (1.25.11)\n",
      "Requirement already satisfied: click<7.2.0,>=7.1.1 in /home/ghost/anaconda3/lib/python3.8/site-packages (from typer<0.4.0,>=0.3.0->spacy) (7.1.2)\n",
      "Requirement already satisfied: MarkupSafe>=0.23 in /home/ghost/anaconda3/lib/python3.8/site-packages (from jinja2->spacy) (1.1.1)\n",
      "\u001b[38;5;3m⚠ As of spaCy v3.0, shortcuts like 'en' are deprecated. Please use the\n",
      "full pipeline package name 'en_core_web_sm' instead.\u001b[0m\n",
      "Collecting en-core-web-sm==3.1.0\n",
      "  Downloading https://github.com/explosion/spacy-models/releases/download/en_core_web_sm-3.1.0/en_core_web_sm-3.1.0-py3-none-any.whl (13.6 MB)\n",
      "\u001b[K     |████████████████████████████████| 13.6 MB 107 kB/s \n",
      "\u001b[?25hRequirement already satisfied: spacy<3.2.0,>=3.1.0 in /home/ghost/anaconda3/lib/python3.8/site-packages (from en-core-web-sm==3.1.0) (3.1.0)\n",
      "Requirement already satisfied: catalogue<2.1.0,>=2.0.4 in /home/ghost/anaconda3/lib/python3.8/site-packages (from spacy<3.2.0,>=3.1.0->en-core-web-sm==3.1.0) (2.0.4)\n",
      "Requirement already satisfied: setuptools in /home/ghost/anaconda3/lib/python3.8/site-packages (from spacy<3.2.0,>=3.1.0->en-core-web-sm==3.1.0) (57.1.0)\n",
      "Requirement already satisfied: packaging>=20.0 in /home/ghost/anaconda3/lib/python3.8/site-packages (from spacy<3.2.0,>=3.1.0->en-core-web-sm==3.1.0) (20.4)\n",
      "Requirement already satisfied: pathy>=0.3.5 in /home/ghost/anaconda3/lib/python3.8/site-packages (from spacy<3.2.0,>=3.1.0->en-core-web-sm==3.1.0) (0.6.0)\n",
      "Requirement already satisfied: srsly<3.0.0,>=2.4.1 in /home/ghost/anaconda3/lib/python3.8/site-packages (from spacy<3.2.0,>=3.1.0->en-core-web-sm==3.1.0) (2.4.1)\n",
      "Requirement already satisfied: requests<3.0.0,>=2.13.0 in /home/ghost/anaconda3/lib/python3.8/site-packages (from spacy<3.2.0,>=3.1.0->en-core-web-sm==3.1.0) (2.24.0)\n",
      "Requirement already satisfied: wasabi<1.1.0,>=0.8.1 in /home/ghost/anaconda3/lib/python3.8/site-packages (from spacy<3.2.0,>=3.1.0->en-core-web-sm==3.1.0) (0.8.2)\n",
      "Requirement already satisfied: typer<0.4.0,>=0.3.0 in /home/ghost/anaconda3/lib/python3.8/site-packages (from spacy<3.2.0,>=3.1.0->en-core-web-sm==3.1.0) (0.3.2)\n",
      "Requirement already satisfied: cymem<2.1.0,>=2.0.2 in /home/ghost/anaconda3/lib/python3.8/site-packages (from spacy<3.2.0,>=3.1.0->en-core-web-sm==3.1.0) (2.0.5)\n",
      "Requirement already satisfied: spacy-legacy<3.1.0,>=3.0.7 in /home/ghost/anaconda3/lib/python3.8/site-packages (from spacy<3.2.0,>=3.1.0->en-core-web-sm==3.1.0) (3.0.8)\n",
      "Requirement already satisfied: pydantic!=1.8,!=1.8.1,<1.9.0,>=1.7.4 in /home/ghost/anaconda3/lib/python3.8/site-packages (from spacy<3.2.0,>=3.1.0->en-core-web-sm==3.1.0) (1.8.2)\n",
      "Requirement already satisfied: murmurhash<1.1.0,>=0.28.0 in /home/ghost/anaconda3/lib/python3.8/site-packages (from spacy<3.2.0,>=3.1.0->en-core-web-sm==3.1.0) (1.0.5)\n",
      "Requirement already satisfied: blis<0.8.0,>=0.4.0 in /home/ghost/anaconda3/lib/python3.8/site-packages (from spacy<3.2.0,>=3.1.0->en-core-web-sm==3.1.0) (0.7.4)\n",
      "Requirement already satisfied: tqdm<5.0.0,>=4.38.0 in /home/ghost/anaconda3/lib/python3.8/site-packages (from spacy<3.2.0,>=3.1.0->en-core-web-sm==3.1.0) (4.50.2)\n",
      "Requirement already satisfied: preshed<3.1.0,>=3.0.2 in /home/ghost/anaconda3/lib/python3.8/site-packages (from spacy<3.2.0,>=3.1.0->en-core-web-sm==3.1.0) (3.0.5)\n",
      "Requirement already satisfied: jinja2 in /home/ghost/anaconda3/lib/python3.8/site-packages (from spacy<3.2.0,>=3.1.0->en-core-web-sm==3.1.0) (2.11.2)\n",
      "Requirement already satisfied: numpy>=1.15.0 in /home/ghost/anaconda3/lib/python3.8/site-packages (from spacy<3.2.0,>=3.1.0->en-core-web-sm==3.1.0) (1.21.0)\n",
      "Requirement already satisfied: thinc<8.1.0,>=8.0.7 in /home/ghost/anaconda3/lib/python3.8/site-packages (from spacy<3.2.0,>=3.1.0->en-core-web-sm==3.1.0) (8.0.7)\n",
      "Requirement already satisfied: pyparsing>=2.0.2 in /home/ghost/anaconda3/lib/python3.8/site-packages (from packaging>=20.0->spacy<3.2.0,>=3.1.0->en-core-web-sm==3.1.0) (2.4.7)\n",
      "Requirement already satisfied: six in /home/ghost/anaconda3/lib/python3.8/site-packages (from packaging>=20.0->spacy<3.2.0,>=3.1.0->en-core-web-sm==3.1.0) (1.15.0)\n",
      "Requirement already satisfied: smart-open<6.0.0,>=5.0.0 in /home/ghost/anaconda3/lib/python3.8/site-packages (from pathy>=0.3.5->spacy<3.2.0,>=3.1.0->en-core-web-sm==3.1.0) (5.1.0)\n",
      "Requirement already satisfied: typing-extensions>=3.7.4.3 in /home/ghost/anaconda3/lib/python3.8/site-packages (from pydantic!=1.8,!=1.8.1,<1.9.0,>=1.7.4->spacy<3.2.0,>=3.1.0->en-core-web-sm==3.1.0) (3.7.4.3)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /home/ghost/anaconda3/lib/python3.8/site-packages (from requests<3.0.0,>=2.13.0->spacy<3.2.0,>=3.1.0->en-core-web-sm==3.1.0) (2020.6.20)\n",
      "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /home/ghost/anaconda3/lib/python3.8/site-packages (from requests<3.0.0,>=2.13.0->spacy<3.2.0,>=3.1.0->en-core-web-sm==3.1.0) (1.25.11)\n",
      "Requirement already satisfied: chardet<4,>=3.0.2 in /home/ghost/anaconda3/lib/python3.8/site-packages (from requests<3.0.0,>=2.13.0->spacy<3.2.0,>=3.1.0->en-core-web-sm==3.1.0) (3.0.4)\n",
      "Requirement already satisfied: idna<3,>=2.5 in /home/ghost/anaconda3/lib/python3.8/site-packages (from requests<3.0.0,>=2.13.0->spacy<3.2.0,>=3.1.0->en-core-web-sm==3.1.0) (2.10)\n",
      "Requirement already satisfied: click<7.2.0,>=7.1.1 in /home/ghost/anaconda3/lib/python3.8/site-packages (from typer<0.4.0,>=0.3.0->spacy<3.2.0,>=3.1.0->en-core-web-sm==3.1.0) (7.1.2)\n",
      "Requirement already satisfied: MarkupSafe>=0.23 in /home/ghost/anaconda3/lib/python3.8/site-packages (from jinja2->spacy<3.2.0,>=3.1.0->en-core-web-sm==3.1.0) (1.1.1)\n",
      "\u001b[38;5;2m✔ Download and installation successful\u001b[0m\n",
      "You can now load the package via spacy.load('en_core_web_sm')\n",
      "\u001b[38;5;3m⚠ As of spaCy v3.0, shortcuts like 'de' are deprecated. Please use the\n",
      "full pipeline package name 'de_core_news_sm' instead.\u001b[0m\n",
      "Collecting de-core-news-sm==3.1.0\n",
      "  Downloading https://github.com/explosion/spacy-models/releases/download/de_core_news_sm-3.1.0/de_core_news_sm-3.1.0-py3-none-any.whl (18.8 MB)\n",
      "\u001b[K     |████████████████████████████████| 18.8 MB 179 kB/s \n",
      "\u001b[?25hRequirement already satisfied: spacy<3.2.0,>=3.1.0 in /home/ghost/anaconda3/lib/python3.8/site-packages (from de-core-news-sm==3.1.0) (3.1.0)\n",
      "Requirement already satisfied: wasabi<1.1.0,>=0.8.1 in /home/ghost/anaconda3/lib/python3.8/site-packages (from spacy<3.2.0,>=3.1.0->de-core-news-sm==3.1.0) (0.8.2)\n",
      "Requirement already satisfied: requests<3.0.0,>=2.13.0 in /home/ghost/anaconda3/lib/python3.8/site-packages (from spacy<3.2.0,>=3.1.0->de-core-news-sm==3.1.0) (2.24.0)\n",
      "Requirement already satisfied: jinja2 in /home/ghost/anaconda3/lib/python3.8/site-packages (from spacy<3.2.0,>=3.1.0->de-core-news-sm==3.1.0) (2.11.2)\n",
      "Requirement already satisfied: cymem<2.1.0,>=2.0.2 in /home/ghost/anaconda3/lib/python3.8/site-packages (from spacy<3.2.0,>=3.1.0->de-core-news-sm==3.1.0) (2.0.5)\n",
      "Requirement already satisfied: thinc<8.1.0,>=8.0.7 in /home/ghost/anaconda3/lib/python3.8/site-packages (from spacy<3.2.0,>=3.1.0->de-core-news-sm==3.1.0) (8.0.7)\n",
      "Requirement already satisfied: murmurhash<1.1.0,>=0.28.0 in /home/ghost/anaconda3/lib/python3.8/site-packages (from spacy<3.2.0,>=3.1.0->de-core-news-sm==3.1.0) (1.0.5)\n",
      "Requirement already satisfied: blis<0.8.0,>=0.4.0 in /home/ghost/anaconda3/lib/python3.8/site-packages (from spacy<3.2.0,>=3.1.0->de-core-news-sm==3.1.0) (0.7.4)\n",
      "Requirement already satisfied: catalogue<2.1.0,>=2.0.4 in /home/ghost/anaconda3/lib/python3.8/site-packages (from spacy<3.2.0,>=3.1.0->de-core-news-sm==3.1.0) (2.0.4)\n",
      "Requirement already satisfied: preshed<3.1.0,>=3.0.2 in /home/ghost/anaconda3/lib/python3.8/site-packages (from spacy<3.2.0,>=3.1.0->de-core-news-sm==3.1.0) (3.0.5)\n",
      "Requirement already satisfied: pathy>=0.3.5 in /home/ghost/anaconda3/lib/python3.8/site-packages (from spacy<3.2.0,>=3.1.0->de-core-news-sm==3.1.0) (0.6.0)\n",
      "Requirement already satisfied: numpy>=1.15.0 in /home/ghost/anaconda3/lib/python3.8/site-packages (from spacy<3.2.0,>=3.1.0->de-core-news-sm==3.1.0) (1.21.0)\n",
      "Requirement already satisfied: srsly<3.0.0,>=2.4.1 in /home/ghost/anaconda3/lib/python3.8/site-packages (from spacy<3.2.0,>=3.1.0->de-core-news-sm==3.1.0) (2.4.1)\n",
      "Requirement already satisfied: typer<0.4.0,>=0.3.0 in /home/ghost/anaconda3/lib/python3.8/site-packages (from spacy<3.2.0,>=3.1.0->de-core-news-sm==3.1.0) (0.3.2)\n",
      "Requirement already satisfied: tqdm<5.0.0,>=4.38.0 in /home/ghost/anaconda3/lib/python3.8/site-packages (from spacy<3.2.0,>=3.1.0->de-core-news-sm==3.1.0) (4.50.2)\n",
      "Requirement already satisfied: pydantic!=1.8,!=1.8.1,<1.9.0,>=1.7.4 in /home/ghost/anaconda3/lib/python3.8/site-packages (from spacy<3.2.0,>=3.1.0->de-core-news-sm==3.1.0) (1.8.2)\n",
      "Requirement already satisfied: spacy-legacy<3.1.0,>=3.0.7 in /home/ghost/anaconda3/lib/python3.8/site-packages (from spacy<3.2.0,>=3.1.0->de-core-news-sm==3.1.0) (3.0.8)\n",
      "Requirement already satisfied: packaging>=20.0 in /home/ghost/anaconda3/lib/python3.8/site-packages (from spacy<3.2.0,>=3.1.0->de-core-news-sm==3.1.0) (20.4)\n",
      "Requirement already satisfied: setuptools in /home/ghost/anaconda3/lib/python3.8/site-packages (from spacy<3.2.0,>=3.1.0->de-core-news-sm==3.1.0) (57.1.0)\n",
      "Requirement already satisfied: pyparsing>=2.0.2 in /home/ghost/anaconda3/lib/python3.8/site-packages (from packaging>=20.0->spacy<3.2.0,>=3.1.0->de-core-news-sm==3.1.0) (2.4.7)\n",
      "Requirement already satisfied: six in /home/ghost/anaconda3/lib/python3.8/site-packages (from packaging>=20.0->spacy<3.2.0,>=3.1.0->de-core-news-sm==3.1.0) (1.15.0)\n",
      "Requirement already satisfied: smart-open<6.0.0,>=5.0.0 in /home/ghost/anaconda3/lib/python3.8/site-packages (from pathy>=0.3.5->spacy<3.2.0,>=3.1.0->de-core-news-sm==3.1.0) (5.1.0)\n",
      "Requirement already satisfied: typing-extensions>=3.7.4.3 in /home/ghost/anaconda3/lib/python3.8/site-packages (from pydantic!=1.8,!=1.8.1,<1.9.0,>=1.7.4->spacy<3.2.0,>=3.1.0->de-core-news-sm==3.1.0) (3.7.4.3)\n",
      "Requirement already satisfied: idna<3,>=2.5 in /home/ghost/anaconda3/lib/python3.8/site-packages (from requests<3.0.0,>=2.13.0->spacy<3.2.0,>=3.1.0->de-core-news-sm==3.1.0) (2.10)\n",
      "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /home/ghost/anaconda3/lib/python3.8/site-packages (from requests<3.0.0,>=2.13.0->spacy<3.2.0,>=3.1.0->de-core-news-sm==3.1.0) (1.25.11)\n",
      "Requirement already satisfied: chardet<4,>=3.0.2 in /home/ghost/anaconda3/lib/python3.8/site-packages (from requests<3.0.0,>=2.13.0->spacy<3.2.0,>=3.1.0->de-core-news-sm==3.1.0) (3.0.4)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /home/ghost/anaconda3/lib/python3.8/site-packages (from requests<3.0.0,>=2.13.0->spacy<3.2.0,>=3.1.0->de-core-news-sm==3.1.0) (2020.6.20)\n",
      "Requirement already satisfied: click<7.2.0,>=7.1.1 in /home/ghost/anaconda3/lib/python3.8/site-packages (from typer<0.4.0,>=0.3.0->spacy<3.2.0,>=3.1.0->de-core-news-sm==3.1.0) (7.1.2)\n",
      "Requirement already satisfied: MarkupSafe>=0.23 in /home/ghost/anaconda3/lib/python3.8/site-packages (from jinja2->spacy<3.2.0,>=3.1.0->de-core-news-sm==3.1.0) (1.1.1)\n",
      "Installing collected packages: de-core-news-sm\n",
      "Successfully installed de-core-news-sm-3.1.0\n",
      "\u001b[38;5;2m✔ Download and installation successful\u001b[0m\n",
      "You can now load the package via spacy.load('de_core_news_sm')\n"
     ]
    }
   ],
   "source": [
    "!pip install git+git://github.com/pytorch/text spacy \n",
    "!python -m spacy download en\n",
    "!python -m spacy download def"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "!export LC_ALL=\"en_US.UTF-8\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "de-en.tgz: 0.00B [00:00, ?B/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "KEk\n",
      "KEK\n",
      "KEK\n",
      "downloading de-en.tgz\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "de-en.tgz: 0.00B [00:00, ?B/s]\n",
      "Failed to download https://wit3.fbk.eu/archive/2016-01//texts/de/en/de-en.tgz\n"
     ]
    },
    {
     "ename": "HTTPError",
     "evalue": "HTTP Error 404: Not Found",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mHTTPError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-32-0efd78893469>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     33\u001b[0m     \u001b[0mMAX_LEN\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m25\u001b[0m  \u001b[0;31m# NOTE: we filter out a lot of sentences for speed\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     34\u001b[0m     \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"KEK\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 35\u001b[0;31m     train_data, valid_data, test_data = datasets.IWSLT.splits(\n\u001b[0m\u001b[1;32m     36\u001b[0m         \u001b[0mexts\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'.de'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'.en'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfields\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mSRC\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mTRG\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     37\u001b[0m         \u001b[0mfilter_pred\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mlambda\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mvars\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'src'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m<=\u001b[0m \u001b[0mMAX_LEN\u001b[0m \u001b[0;32mand\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.8/site-packages/torchtext/legacy/datasets/translation.py\u001b[0m in \u001b[0;36msplits\u001b[0;34m(cls, exts, fields, root, train, validation, test, **kwargs)\u001b[0m\n\u001b[1;32m    145\u001b[0m         \u001b[0mcls\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0murls\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mcls\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbase_url\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mexts\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mexts\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcls\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdirname\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    146\u001b[0m         \u001b[0mcheck\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mjoin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mroot\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcls\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcls\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdirname\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 147\u001b[0;31m         \u001b[0mpath\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcls\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdownload\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mroot\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcheck\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mcheck\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    148\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    149\u001b[0m         \u001b[0mtrain\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m'.'\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mjoin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mtrain\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcls\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdirname\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.8/site-packages/torchtext/legacy/data/dataset.py\u001b[0m in \u001b[0;36mdownload\u001b[0;34m(cls, root, check)\u001b[0m\n\u001b[1;32m    180\u001b[0m                         \u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmakedirs\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdirname\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mzpath\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    181\u001b[0m                     \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'downloading {}'\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfilename\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 182\u001b[0;31m                     \u001b[0mdownload_from_url\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0murl\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mzpath\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    183\u001b[0m                 \u001b[0mzroot\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mext\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msplitext\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mzpath\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    184\u001b[0m                 \u001b[0m_\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mext_inner\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msplitext\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mzroot\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.8/site-packages/torchtext/utils.py\u001b[0m in \u001b[0;36mdownload_from_url\u001b[0;34m(url, path, root, overwrite, hash_value, hash_type)\u001b[0m\n\u001b[1;32m    113\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    114\u001b[0m     \u001b[0;31m# download data and move to path\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 115\u001b[0;31m     \u001b[0m_DATASET_DOWNLOAD_MANAGER\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_local_path\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0murl\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdestination\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    116\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    117\u001b[0m     \u001b[0mlogging\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0minfo\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'File {} downloaded.'\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.8/site-packages/iopath/common/file_io.py\u001b[0m in \u001b[0;36mget_local_path\u001b[0;34m(self, path, force, **kwargs)\u001b[0m\n\u001b[1;32m   1195\u001b[0m         \u001b[0mhandler\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__get_path_handler\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# type: ignore\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1196\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1197\u001b[0;31m             \u001b[0mbret\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mhandler\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_get_local_path\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mforce\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mforce\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1198\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0mTypeError\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1199\u001b[0m             \u001b[0mbret\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mhandler\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_get_local_path\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.8/site-packages/torchtext/_download_hooks.py\u001b[0m in \u001b[0;36m_get_local_path\u001b[0;34m(self, path, force, cache_dir, **kwargs)\u001b[0m\n\u001b[1;32m    148\u001b[0m         \u001b[0mdestination\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mkwargs\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"destination\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    149\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 150\u001b[0;31m         \u001b[0mlocal_path\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpath_manager\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_local_path\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mforce\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    151\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    152\u001b[0m         \u001b[0mshutil\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmove\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlocal_path\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdestination\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.8/site-packages/iopath/common/file_io.py\u001b[0m in \u001b[0;36mget_local_path\u001b[0;34m(self, path, force, **kwargs)\u001b[0m\n\u001b[1;32m   1195\u001b[0m         \u001b[0mhandler\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__get_path_handler\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# type: ignore\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1196\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1197\u001b[0;31m             \u001b[0mbret\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mhandler\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_get_local_path\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mforce\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mforce\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1198\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0mTypeError\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1199\u001b[0m             \u001b[0mbret\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mhandler\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_get_local_path\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.8/site-packages/iopath/common/file_io.py\u001b[0m in \u001b[0;36m_get_local_path\u001b[0;34m(self, path, force, cache_dir, **kwargs)\u001b[0m\n\u001b[1;32m    795\u001b[0m                 \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0misfile\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcached\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    796\u001b[0m                     \u001b[0mlogger\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0minfo\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Downloading {} ...\"\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 797\u001b[0;31m                     \u001b[0mcached\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdownload\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdirname\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfilename\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mfilename\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    798\u001b[0m             \u001b[0mlogger\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0minfo\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"URL {} cached in {}\"\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcached\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    799\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcache_map\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcached\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.8/site-packages/iopath/common/download.py\u001b[0m in \u001b[0;36mdownload\u001b[0;34m(url, dir, filename, progress)\u001b[0m\n\u001b[1;32m     56\u001b[0m                 \u001b[0munit\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\"B\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0munit_scale\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mminiters\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdesc\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mfilename\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mleave\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     57\u001b[0m             ) as t:\n\u001b[0;32m---> 58\u001b[0;31m                 \u001b[0mtmp\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0m_\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mrequest\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0murlretrieve\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0murl\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfilename\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtmp\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mreporthook\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mhook\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mt\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     59\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     60\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.8/urllib/request.py\u001b[0m in \u001b[0;36murlretrieve\u001b[0;34m(url, filename, reporthook, data)\u001b[0m\n\u001b[1;32m    245\u001b[0m     \u001b[0murl_type\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpath\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_splittype\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0murl\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    246\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 247\u001b[0;31m     \u001b[0;32mwith\u001b[0m \u001b[0mcontextlib\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mclosing\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0murlopen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0murl\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdata\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mfp\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    248\u001b[0m         \u001b[0mheaders\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0minfo\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    249\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.8/urllib/request.py\u001b[0m in \u001b[0;36murlopen\u001b[0;34m(url, data, timeout, cafile, capath, cadefault, context)\u001b[0m\n\u001b[1;32m    220\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    221\u001b[0m         \u001b[0mopener\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_opener\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 222\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0mopener\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mopen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0murl\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdata\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtimeout\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    223\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    224\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0minstall_opener\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mopener\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.8/urllib/request.py\u001b[0m in \u001b[0;36mopen\u001b[0;34m(self, fullurl, data, timeout)\u001b[0m\n\u001b[1;32m    529\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mprocessor\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mprocess_response\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mprotocol\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    530\u001b[0m             \u001b[0mmeth\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mgetattr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mprocessor\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmeth_name\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 531\u001b[0;31m             \u001b[0mresponse\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmeth\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mreq\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mresponse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    532\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    533\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mresponse\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.8/urllib/request.py\u001b[0m in \u001b[0;36mhttp_response\u001b[0;34m(self, request, response)\u001b[0m\n\u001b[1;32m    638\u001b[0m         \u001b[0;31m# request was successfully received, understood, and accepted.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    639\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0;36m200\u001b[0m \u001b[0;34m<=\u001b[0m \u001b[0mcode\u001b[0m \u001b[0;34m<\u001b[0m \u001b[0;36m300\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 640\u001b[0;31m             response = self.parent.error(\n\u001b[0m\u001b[1;32m    641\u001b[0m                 'http', request, response, code, msg, hdrs)\n\u001b[1;32m    642\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.8/urllib/request.py\u001b[0m in \u001b[0;36merror\u001b[0;34m(self, proto, *args)\u001b[0m\n\u001b[1;32m    567\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mhttp_err\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    568\u001b[0m             \u001b[0margs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mdict\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'default'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'http_error_default'\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0morig_args\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 569\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call_chain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    570\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    571\u001b[0m \u001b[0;31m# XXX probably also want an abstract factory that knows when it makes\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.8/urllib/request.py\u001b[0m in \u001b[0;36m_call_chain\u001b[0;34m(self, chain, kind, meth_name, *args)\u001b[0m\n\u001b[1;32m    500\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mhandler\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mhandlers\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    501\u001b[0m             \u001b[0mfunc\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mgetattr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mhandler\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmeth_name\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 502\u001b[0;31m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    503\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mresult\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    504\u001b[0m                 \u001b[0;32mreturn\u001b[0m \u001b[0mresult\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.8/urllib/request.py\u001b[0m in \u001b[0;36mhttp_error_default\u001b[0;34m(self, req, fp, code, msg, hdrs)\u001b[0m\n\u001b[1;32m    647\u001b[0m \u001b[0;32mclass\u001b[0m \u001b[0mHTTPDefaultErrorHandler\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mBaseHandler\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    648\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mhttp_error_default\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mreq\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfp\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcode\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmsg\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhdrs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 649\u001b[0;31m         \u001b[0;32mraise\u001b[0m \u001b[0mHTTPError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mreq\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfull_url\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcode\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmsg\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhdrs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfp\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    650\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    651\u001b[0m \u001b[0;32mclass\u001b[0m \u001b[0mHTTPRedirectHandler\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mBaseHandler\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mHTTPError\u001b[0m: HTTP Error 404: Not Found"
     ]
    }
   ],
   "source": [
    "from torchtext import data, datasets\n",
    "from torchtext.legacy import data\n",
    "from torchtext.legacy import datasets\n",
    "\n",
    "if True:\n",
    "    import spacy\n",
    "    spacy_de = spacy.load(\"de_core_news_sm\")\n",
    "    spacy_en = spacy.load(\"en_core_web_sm\")\n",
    "\n",
    "    def tokenize_de(text):\n",
    "        return [tok.text for tok in spacy_de.tokenizer(text)]\n",
    "\n",
    "    def tokenize_en(text):\n",
    "        return [tok.text for tok in spacy_en.tokenizer(text)]\n",
    "\n",
    "    print(\"KEk\")\n",
    "\n",
    "    UNK_TOKEN = \"<unk>\"\n",
    "    PAD_TOKEN = \"<pad>\"    \n",
    "    SOS_TOKEN = \"<s>\"\n",
    "    EOS_TOKEN = \"</s>\"\n",
    "    LOWER = True\n",
    "    \n",
    "    # we include lengths to provide to the RNNs\n",
    "    SRC = data.Field(tokenize=tokenize_de, \n",
    "                     batch_first=True, lower=LOWER, include_lengths=True,\n",
    "                     unk_token=UNK_TOKEN, pad_token=PAD_TOKEN, init_token=None, eos_token=EOS_TOKEN)\n",
    "    TRG = data.Field(tokenize=tokenize_en, \n",
    "                     batch_first=True, lower=LOWER, include_lengths=True,\n",
    "                     unk_token=UNK_TOKEN, pad_token=PAD_TOKEN, init_token=SOS_TOKEN, eos_token=EOS_TOKEN)\n",
    "\n",
    "    print(\"KEK\")\n",
    "    MAX_LEN = 25  # NOTE: we filter out a lot of sentences for speed\n",
    "    print(\"KEK\")\n",
    "    train_data, valid_data, test_data = datasets.IWSLT.splits(\n",
    "        exts=('.de', '.en'), fields=(SRC, TRG), \n",
    "        filter_pred=lambda x: len(vars(x)['src']) <= MAX_LEN and \n",
    "            len(vars(x)['trg']) <= MAX_LEN) \n",
    "    print(\"KEK\")\n",
    "    MIN_FREQ = 5  # NOTE: we limit the vocabulary to frequent words for speed\n",
    "    SRC.build_vocab(train_data.src, min_freq=MIN_FREQ)\n",
    "    TRG.build_vocab(train_data.trg, min_freq=MIN_FREQ)\n",
    "    print(\"KEK\")\n",
    "    \n",
    "    PAD_INDEX = TRG.vocab.stoi[PAD_TOKEN]\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "df = pd.read_csv('')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<spacy.lang.de.German object at 0x7ff723cb4d30>\n"
     ]
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'train_data' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-33-87220556ae5c>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     27\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     28\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 29\u001b[0;31m \u001b[0mprint_data_info\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrain_data\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvalid_data\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtest_data\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mSRC\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mTRG\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m: name 'train_data' is not defined"
     ]
    }
   ],
   "source": [
    "def print_data_info(train_data, valid_data, test_data, src_field, trg_field):\n",
    "    \"\"\" This prints some useful stuff about our data sets. \"\"\"\n",
    "\n",
    "    print(\"Data set sizes (number of sentence pairs):\")\n",
    "    print('train', len(train_data))\n",
    "    print('valid', len(valid_data))\n",
    "    print('test', len(test_data), \"\\n\")\n",
    "\n",
    "    print(\"First training example:\")\n",
    "    print(\"src:\", \" \".join(vars(train_data[0])['src']))\n",
    "    print(\"trg:\", \" \".join(vars(train_data[0])['trg']), \"\\n\")\n",
    "\n",
    "    print(\"Most common words (src):\")\n",
    "    print(\"\\n\".join([\"%10s %10d\" % x for x in src_field.vocab.freqs.most_common(10)]), \"\\n\")\n",
    "    print(\"Most common words (trg):\")\n",
    "    print(\"\\n\".join([\"%10s %10d\" % x for x in trg_field.vocab.freqs.most_common(10)]), \"\\n\")\n",
    "\n",
    "    print(\"First 10 words (src):\")\n",
    "    print(\"\\n\".join(\n",
    "        '%02d %s' % (i, t) for i, t in enumerate(src_field.vocab.itos[:10])), \"\\n\")\n",
    "    print(\"First 10 words (trg):\")\n",
    "    print(\"\\n\".join(\n",
    "        '%02d %s' % (i, t) for i, t in enumerate(trg_field.vocab.itos[:10])), \"\\n\")\n",
    "\n",
    "    print(\"Number of German words (types):\", len(src_field.vocab))\n",
    "    print(\"Number of English words (types):\", len(trg_field.vocab), \"\\n\")\n",
    "    \n",
    "    \n",
    "print_data_info(train_data, valid_data, test_data, SRC, TRG)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.8.5 64-bit ('base': conda)",
   "name": "python385jvsc74a57bd009370de619b8b9149cc7c753872ea6aa5871e135f833321ec46ee8dac5b2b335"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  },
  "orig_nbformat": 2
 },
 "nbformat": 4,
 "nbformat_minor": 2
}