{
 "cells": [
  {
   "cell_type": "raw",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CUDA: True\n",
      "cuda:0\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import math, copy, time\n",
    "import matplotlib.pyplot as plt\n",
    "from torch.nn.utils.rnn import pack_padded_sequence, pad_packed_sequence\n",
    "from IPython.core.debugger import set_trace\n",
    "\n",
    "# we will use CUDA if it is available\n",
    "USE_CUDA = torch.cuda.is_available()\n",
    "DEVICE=torch.device('cuda:0') # or set to 'cpu'\n",
    "print(\"CUDA:\", USE_CUDA)\n",
    "print(DEVICE)\n",
    "\n",
    "seed = 42\n",
    "np.random.seed(seed)\n",
    "\n",
    "torch.manual_seed(seed)\n",
    "torch.cuda.manual_seed(seed)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "class EncoderDecoder(nn.Module):\n",
    "    \"\"\"\n",
    "    A standard Encoder-Decoder architecture. Base for this and many \n",
    "    other models.\n",
    "    \"\"\"\n",
    "    def __init__(self, encoder, decoder, src_embed, trg_embed, generator):\n",
    "        super(EncoderDecoder, self).__init__()\n",
    "        self.encoder = encoder\n",
    "        self.decoder = decoder\n",
    "        self.src_embed = src_embed\n",
    "        self.trg_embed = trg_embed\n",
    "        self.generator = generator\n",
    "        \n",
    "    def forward(self, src, trg, src_mask, trg_mask, src_lengths, trg_lengths):\n",
    "        \"\"\"Take in and process masked src and target sequences.\"\"\"\n",
    "        encoder_hidden, encoder_final = self.encode(src, src_mask, src_lengths)\n",
    "        return self.decode(encoder_hidden, encoder_final, src_mask, trg, trg_mask)\n",
    "    \n",
    "    def encode(self, src, src_mask, src_lengths):\n",
    "        return self.encoder(self.src_embed(src), src_mask, src_lengths)\n",
    "    \n",
    "    def decode(self, encoder_hidden, encoder_final, src_mask, trg, trg_mask,\n",
    "               decoder_hidden=None):\n",
    "        return self.decoder(self.trg_embed(trg), encoder_hidden, encoder_final,\n",
    "                            src_mask, trg_mask, hidden=decoder_hidden)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Generator(nn.Module):\n",
    "    \"\"\"Define standard linear + softmax generation step.\"\"\"\n",
    "    def __init__(self, hidden_size, vocab_size):\n",
    "        super(Generator, self).__init__()\n",
    "        self.proj = nn.Linear(hidden_size, vocab_size, bias=False)\n",
    "\n",
    "    def forward(self, x):\n",
    "        return F.log_softmax(self.proj(x), dim=-1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Encoder(nn.Module):\n",
    "    \"\"\"Encodes a sequence of word embeddings\"\"\"\n",
    "    def __init__(self, input_size, hidden_size, num_layers=1, dropout=0.):\n",
    "        super(Encoder, self).__init__()\n",
    "        self.num_layers = num_layers\n",
    "        self.rnn = nn.GRU(input_size, hidden_size, num_layers, \n",
    "                          batch_first=True, bidirectional=True, dropout=dropout)\n",
    "        \n",
    "    def forward(self, x, mask, lengths):\n",
    "        \"\"\"\n",
    "        Applies a bidirectional GRU to sequence of embeddings x.\n",
    "        The input mini-batch x needs to be sorted by length.\n",
    "        x should have dimensions [batch, time, dim].\n",
    "        \"\"\"\n",
    "        packed = pack_padded_sequence(x, lengths, batch_first=True)\n",
    "        output, final = self.rnn(packed)\n",
    "        output, _ = pad_packed_sequence(output, batch_first=True)\n",
    "\n",
    "        # we need to manually concatenate the final states for both directions\n",
    "        fwd_final = final[0:final.size(0):2]\n",
    "        bwd_final = final[1:final.size(0):2]\n",
    "        final = torch.cat([fwd_final, bwd_final], dim=2)  # [num_layers, batch, 2*dim]\n",
    "\n",
    "        return output, final\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Decoder(nn.Module):\n",
    "    \"\"\"A conditional RNN decoder with attention.\"\"\"\n",
    "    \n",
    "    def __init__(self, emb_size, hidden_size, attention, num_layers=1, dropout=0.5,\n",
    "                 bridge=True):\n",
    "        super(Decoder, self).__init__()\n",
    "        \n",
    "        self.hidden_size = hidden_size\n",
    "        self.num_layers = num_layers\n",
    "        self.attention = attention\n",
    "        self.dropout = dropout\n",
    "                 \n",
    "        self.rnn = nn.GRU(emb_size + 2 * hidden_size, hidden_size, num_layers,\n",
    "                          batch_first=True, dropout=dropout)\n",
    "                 \n",
    "        # to initialize from the final encoder state\n",
    "        self.bridge = nn.Linear(2 * hidden_size, hidden_size, bias=True) if bridge else None\n",
    "\n",
    "        self.dropout_layer = nn.Dropout(p=dropout)\n",
    "        self.pre_output_layer = nn.Linear(hidden_size + 2 * hidden_size + emb_size,\n",
    "                                          hidden_size, bias=False)\n",
    "        \n",
    "    def forward_step(self, prev_embed, encoder_hidden, src_mask, proj_key, hidden):\n",
    "        \"\"\"Perform a single decoder step (1 word)\"\"\"\n",
    "\n",
    "        # compute context vector using attention mechanism\n",
    "        query = hidden[-1].unsqueeze(1)  # [#layers, B, D] -> [B, 1, D]\n",
    "        context, attn_probs = self.attention(\n",
    "            query=query, proj_key=proj_key,\n",
    "            value=encoder_hidden, mask=src_mask)\n",
    "\n",
    "        # update rnn hidden state\n",
    "        rnn_input = torch.cat([prev_embed, context], dim=2)\n",
    "        output, hidden = self.rnn(rnn_input, hidden)\n",
    "        \n",
    "        pre_output = torch.cat([prev_embed, output, context], dim=2)\n",
    "        pre_output = self.dropout_layer(pre_output)\n",
    "        pre_output = self.pre_output_layer(pre_output)\n",
    "\n",
    "        return output, hidden, pre_output\n",
    "    \n",
    "    def forward(self, trg_embed, encoder_hidden, encoder_final, \n",
    "                src_mask, trg_mask, hidden=None, max_len=None):\n",
    "        \"\"\"Unroll the decoder one step at a time.\"\"\"\n",
    "                                         \n",
    "        # the maximum number of steps to unroll the RNN\n",
    "        if max_len is None:\n",
    "            max_len = trg_mask.size(-1)\n",
    "\n",
    "        # initialize decoder hidden state\n",
    "        if hidden is None:\n",
    "            hidden = self.init_hidden(encoder_final)\n",
    "        \n",
    "        # pre-compute projected encoder hidden states\n",
    "        # (the \"keys\" for the attention mechanism)\n",
    "        # this is only done for efficiency\n",
    "        proj_key = self.attention.key_layer(encoder_hidden)\n",
    "        \n",
    "        # here we store all intermediate hidden states and pre-output vectors\n",
    "        decoder_states = []\n",
    "        pre_output_vectors = []\n",
    "        \n",
    "        # unroll the decoder RNN for max_len steps\n",
    "        for i in range(max_len):\n",
    "            prev_embed = trg_embed[:, i].unsqueeze(1)\n",
    "            output, hidden, pre_output = self.forward_step(\n",
    "              prev_embed, encoder_hidden, src_mask, proj_key, hidden)\n",
    "            decoder_states.append(output)\n",
    "            pre_output_vectors.append(pre_output)\n",
    "\n",
    "        decoder_states = torch.cat(decoder_states, dim=1)\n",
    "        pre_output_vectors = torch.cat(pre_output_vectors, dim=1)\n",
    "        return decoder_states, hidden, pre_output_vectors  # [B, N, D]\n",
    "\n",
    "    def init_hidden(self, encoder_final):\n",
    "        \"\"\"Returns the initial decoder state,\n",
    "        conditioned on the final encoder state.\"\"\"\n",
    "\n",
    "        if encoder_final is None:\n",
    "            return None  # start with zeros\n",
    "\n",
    "        return torch.tanh(self.bridge(encoder_final))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "class BahdanauAttention(nn.Module):\n",
    "    \"\"\"Implements Bahdanau (MLP) attention\"\"\"\n",
    "    \n",
    "    def __init__(self, hidden_size, key_size=None, query_size=None):\n",
    "        super(BahdanauAttention, self).__init__()\n",
    "        \n",
    "        # We assume a bi-directional encoder so key_size is 2*hidden_size\n",
    "        key_size = 2 * hidden_size if key_size is None else key_size\n",
    "        query_size = hidden_size if query_size is None else query_size\n",
    "\n",
    "        self.key_layer = nn.Linear(key_size, hidden_size, bias=False)\n",
    "        self.query_layer = nn.Linear(query_size, hidden_size, bias=False)\n",
    "        self.energy_layer = nn.Linear(hidden_size, 1, bias=False)\n",
    "        \n",
    "        # to store attention scores\n",
    "        self.alphas = None\n",
    "        \n",
    "    def forward(self, query=None, proj_key=None, value=None, mask=None):\n",
    "        assert mask is not None, \"mask is required\"\n",
    "\n",
    "        # We first project the query (the decoder state).\n",
    "        # The projected keys (the encoder states) were already pre-computated.\n",
    "        query = self.query_layer(query)\n",
    "        \n",
    "        # Calculate scores.\n",
    "        scores = self.energy_layer(torch.tanh(query + proj_key))\n",
    "        scores = scores.squeeze(2).unsqueeze(1)\n",
    "        \n",
    "        # Mask out invalid positions.\n",
    "        # The mask marks valid positions so we invert it using `mask & 0`.\n",
    "        scores.data.masked_fill_(mask == 0, -float('inf'))\n",
    "        \n",
    "        # Turn scores to probabilities.\n",
    "        alphas = F.softmax(scores, dim=-1)\n",
    "        self.alphas = alphas        \n",
    "        \n",
    "        # The context vector is the weighted sum of the values.\n",
    "        context = torch.bmm(alphas, value)\n",
    "        \n",
    "        # context shape: [B, 1, 2D], alphas shape: [B, 1, M]\n",
    "        return context, alphas\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_model(src_vocab, tgt_vocab, emb_size=256, hidden_size=512, num_layers=1, dropout=0.1):\n",
    "    \"Helper: Construct a model from hyperparameters.\"\n",
    "\n",
    "    attention = BahdanauAttention(hidden_size)\n",
    "\n",
    "    model = EncoderDecoder(\n",
    "        Encoder(emb_size, hidden_size, num_layers=num_layers, dropout=dropout),\n",
    "        Decoder(emb_size, hidden_size, attention, num_layers=num_layers, dropout=dropout),\n",
    "        nn.Embedding(src_vocab, emb_size),\n",
    "        nn.Embedding(tgt_vocab, emb_size),\n",
    "        Generator(hidden_size, tgt_vocab))\n",
    "\n",
    "    return model.cuda() if USE_CUDA else model\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Batch:\n",
    "    \"\"\"Object for holding a batch of data with mask during training.\n",
    "    Input is a batch from a torch text iterator.\n",
    "    \"\"\"\n",
    "    def __init__(self, src, trg, pad_index=0):\n",
    "        \n",
    "        src, src_lengths = src\n",
    "        \n",
    "        self.src = src\n",
    "        self.src_lengths = src_lengths\n",
    "        self.src_mask = (src != pad_index).unsqueeze(-2)\n",
    "        self.nseqs = src.size(0)\n",
    "        \n",
    "        self.trg = None\n",
    "        self.trg_y = None\n",
    "        self.trg_mask = None\n",
    "        self.trg_lengths = None\n",
    "        self.ntokens = None\n",
    "\n",
    "        if trg is not None:\n",
    "            trg, trg_lengths = trg\n",
    "            self.trg = trg[:, :-1]\n",
    "            self.trg_lengths = trg_lengths\n",
    "            self.trg_y = trg[:, 1:]\n",
    "            self.trg_mask = (self.trg_y != pad_index)\n",
    "            self.ntokens = (self.trg_y != pad_index).data.sum().item()\n",
    "        \n",
    "        if USE_CUDA:\n",
    "            self.src = self.src.cuda()\n",
    "            self.src_mask = self.src_mask.cuda()\n",
    "\n",
    "            if trg is not None:\n",
    "                self.trg = self.trg.cuda()\n",
    "                self.trg_y = self.trg_y.cuda()\n",
    "                self.trg_mask = self.trg_mask.cuda()\n",
    "                \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_epoch(data_iter, model, loss_compute, print_every=50):\n",
    "    \"\"\"Standard Training and Logging Function\"\"\"\n",
    "\n",
    "    start = time.time()\n",
    "    total_tokens = 0\n",
    "    total_loss = 0\n",
    "    print_tokens = 0\n",
    "\n",
    "    for i, batch in enumerate(data_iter, 1):\n",
    "        \n",
    "        out, _, pre_output = model.forward(batch.src, batch.trg,\n",
    "                                           batch.src_mask, batch.trg_mask,\n",
    "                                           batch.src_lengths, batch.trg_lengths)\n",
    "        loss = loss_compute(pre_output, batch.trg_y, batch.nseqs)\n",
    "        total_loss += loss\n",
    "        total_tokens += batch.ntokens\n",
    "        print_tokens += batch.ntokens\n",
    "        \n",
    "        if model.training and i % print_every == 0:\n",
    "            elapsed = time.time() - start\n",
    "            print(\"Epoch Step: %d Loss: %f Tokens per Sec: %f\" %\n",
    "                    (i, loss / batch.nseqs, print_tokens / elapsed))\n",
    "            start = time.time()\n",
    "            print_tokens = 0\n",
    "\n",
    "    return math.exp(total_loss / float(total_tokens))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def data_gen(num_words=11, batch_size=16, num_batches=100, length=10, pad_index=0, sos_index=1):\n",
    "    \"\"\"Generate random data for a src-tgt copy task.\"\"\"\n",
    "    for i in range(num_batches):\n",
    "        data = torch.from_numpy(\n",
    "          np.random.randint(1, num_words, size=(batch_size, length)))\n",
    "        data[:, 0] = sos_index\n",
    "        data = data.cuda() if USE_CUDA else data\n",
    "        src = data[:, 1:]\n",
    "        trg = data\n",
    "        src_lengths = [length-1] * batch_size\n",
    "        trg_lengths = [length] * batch_size\n",
    "        yield Batch((src, src_lengths), (trg, trg_lengths), pad_index=pad_index)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SimpleLossCompute:\n",
    "    \"\"\"A simple loss compute and train function.\"\"\"\n",
    "\n",
    "    def __init__(self, generator, criterion, opt=None):\n",
    "        self.generator = generator\n",
    "        self.criterion = criterion\n",
    "        self.opt = opt\n",
    "\n",
    "    def __call__(self, x, y, norm):\n",
    "        x = self.generator(x)\n",
    "        loss = self.criterion(x.contiguous().view(-1, x.size(-1)),\n",
    "                              y.contiguous().view(-1))\n",
    "        loss = loss / norm\n",
    "\n",
    "        if self.opt is not None:\n",
    "            loss.backward()          \n",
    "            self.opt.step()\n",
    "            self.opt.zero_grad()\n",
    "\n",
    "        return loss.data.item() * norm\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def greedy_decode(model, src, src_mask, src_lengths, max_len=100, sos_index=1, eos_index=None):\n",
    "    \"\"\"Greedily decode a sentence.\"\"\"\n",
    "\n",
    "    with torch.no_grad():\n",
    "        encoder_hidden, encoder_final = model.encode(src, src_mask, src_lengths)\n",
    "        prev_y = torch.ones(1, 1).fill_(sos_index).type_as(src)\n",
    "        trg_mask = torch.ones_like(prev_y)\n",
    "\n",
    "    output = []\n",
    "    attention_scores = []\n",
    "    hidden = None\n",
    "\n",
    "    for i in range(max_len):\n",
    "        with torch.no_grad():\n",
    "            out, hidden, pre_output = model.decode(\n",
    "              encoder_hidden, encoder_final, src_mask,\n",
    "              prev_y, trg_mask, hidden)\n",
    "\n",
    "            # we predict from the pre-output layer, which is\n",
    "            # a combination of Decoder state, prev emb, and context\n",
    "            prob = model.generator(pre_output[:, -1])\n",
    "\n",
    "        _, next_word = torch.max(prob, dim=1)\n",
    "        next_word = next_word.data.item()\n",
    "        output.append(next_word)\n",
    "        prev_y = torch.ones(1, 1).type_as(src).fill_(next_word)\n",
    "        attention_scores.append(model.decoder.attention.alphas.cpu().numpy())\n",
    "    \n",
    "    output = np.array(output)\n",
    "        \n",
    "    # cut off everything starting from </s> \n",
    "    # (only when eos_index provided)\n",
    "    if eos_index is not None:\n",
    "        first_eos = np.where(output==eos_index)[0]\n",
    "        if len(first_eos) > 0:\n",
    "            output = output[:first_eos[0]]      \n",
    "    \n",
    "    return output, np.concatenate(attention_scores, axis=1)\n",
    "  \n",
    "\n",
    "def lookup_words(x, vocab=None):\n",
    "    if vocab is not None:\n",
    "        x = [vocab.itos[i] for i in x]\n",
    "\n",
    "    return [str(t) for t in x]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def print_examples(example_iter, model, n=2, max_len=100, \n",
    "                   sos_index=1, \n",
    "                   src_eos_index=None, \n",
    "                   trg_eos_index=None, \n",
    "                   src_vocab=None, trg_vocab=None):\n",
    "    \"\"\"Prints N examples. Assumes batch size of 1.\"\"\"\n",
    "\n",
    "    model.eval()\n",
    "    count = 0\n",
    "    print()\n",
    "    \n",
    "    if src_vocab is not None and trg_vocab is not None:\n",
    "        src_eos_index = src_vocab.stoi[EOS_TOKEN]\n",
    "        trg_sos_index = trg_vocab.stoi[SOS_TOKEN]\n",
    "        trg_eos_index = trg_vocab.stoi[EOS_TOKEN]\n",
    "    else:\n",
    "        src_eos_index = None\n",
    "        trg_sos_index = 1\n",
    "        trg_eos_index = None\n",
    "        \n",
    "    for i, batch in enumerate(example_iter):\n",
    "      \n",
    "        src = batch.src.cpu().numpy()[0, :]\n",
    "        trg = batch.trg_y.cpu().numpy()[0, :]\n",
    "\n",
    "        # remove </s> (if it is there)\n",
    "        src = src[:-1] if src[-1] == src_eos_index else src\n",
    "        trg = trg[:-1] if trg[-1] == trg_eos_index else trg      \n",
    "      \n",
    "        result, _ = greedy_decode(\n",
    "          model, batch.src, batch.src_mask, batch.src_lengths,\n",
    "          max_len=max_len, sos_index=trg_sos_index, eos_index=trg_eos_index)\n",
    "        print(\"Example #%d\" % (i+1))\n",
    "        print(\"Src : \", \" \".join(lookup_words(src, vocab=src_vocab)))\n",
    "        print(\"Trg : \", \" \".join(lookup_words(trg, vocab=trg_vocab)))\n",
    "        print(\"Pred: \", \" \".join(lookup_words(result, vocab=trg_vocab)))\n",
    "        print()\n",
    "        \n",
    "        count += 1\n",
    "        if count == n:\n",
    "            break\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_copy_task():\n",
    "    \"\"\"Train the simple copy task.\"\"\"\n",
    "    num_words = 11\n",
    "    criterion = nn.NLLLoss(reduction=\"sum\", ignore_index=0)\n",
    "    model = make_model(num_words, num_words, emb_size=32, hidden_size=64)\n",
    "    optim = torch.optim.Adam(model.parameters(), lr=0.0003)\n",
    "    eval_data = list(data_gen(num_words=num_words, batch_size=1, num_batches=100))\n",
    " \n",
    "    dev_perplexities = []\n",
    "    \n",
    "    if USE_CUDA:\n",
    "        model.cuda()\n",
    "\n",
    "    for epoch in range(10):\n",
    "        \n",
    "        print(\"Epoch %d\" % epoch)\n",
    "\n",
    "        # train\n",
    "        model.train()\n",
    "        data = data_gen(num_words=num_words, batch_size=32, num_batches=100)\n",
    "        run_epoch(data, model,\n",
    "                  SimpleLossCompute(model.generator, criterion, optim))\n",
    "\n",
    "        # evaluate\n",
    "        model.eval()\n",
    "        with torch.no_grad(): \n",
    "            perplexity = run_epoch(eval_data, model,\n",
    "                                   SimpleLossCompute(model.generator, criterion, None))\n",
    "            print(\"Evaluation perplexity: %f\" % perplexity)\n",
    "            dev_perplexities.append(perplexity)\n",
    "            print_examples(eval_data, model, n=2, max_len=9)\n",
    "        \n",
    "    return dev_perplexities\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ghost/anaconda3/lib/python3.8/site-packages/torch/nn/modules/rnn.py:62: UserWarning: dropout option adds dropout after all but last recurrent layer, so non-zero dropout expects num_layers greater than 1, but got dropout=0.1 and num_layers=1\n",
      "  warnings.warn(\"dropout option adds dropout after all but last \"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0\n",
      "Epoch Step: 50 Loss: 19.720032 Tokens per Sec: 13314.147152\n",
      "Epoch Step: 100 Loss: 17.850552 Tokens per Sec: 18832.917925\n",
      "Evaluation perplexity: 7.165516\n",
      "\n",
      "Example #1\n",
      "Src :  4 8 5 7 10 3 7 8 5\n",
      "Trg :  4 8 5 7 10 3 7 8 5\n",
      "Pred:  5 8 7 5 8 7 5 8 7\n",
      "\n",
      "Example #2\n",
      "Src :  8 8 3 6 5 2 8 6 2\n",
      "Trg :  8 8 3 6 5 2 8 6 2\n",
      "Pred:  8 8 8 8 8 8 8 8 8\n",
      "\n",
      "Epoch 1\n",
      "Epoch Step: 50 Loss: 15.429652 Tokens per Sec: 19106.808294\n",
      "Epoch Step: 100 Loss: 11.758204 Tokens per Sec: 18936.812138\n",
      "Evaluation perplexity: 3.761093\n",
      "\n",
      "Example #1\n",
      "Src :  4 8 5 7 10 3 7 8 5\n",
      "Trg :  4 8 5 7 10 3 7 8 5\n",
      "Pred:  4 8 5 7 5 3 8 7 5\n",
      "\n",
      "Example #2\n",
      "Src :  8 8 3 6 5 2 8 6 2\n",
      "Trg :  8 8 3 6 5 2 8 6 2\n",
      "Pred:  8 8 3 8 2 5 8 3 2\n",
      "\n",
      "Epoch 2\n",
      "Epoch Step: 50 Loss: 9.917424 Tokens per Sec: 18803.309120\n",
      "Epoch Step: 100 Loss: 8.949947 Tokens per Sec: 21505.766551\n",
      "Evaluation perplexity: 2.573576\n",
      "\n",
      "Example #1\n",
      "Src :  4 8 5 7 10 3 7 8 5\n",
      "Trg :  4 8 5 7 10 3 7 8 5\n",
      "Pred:  4 8 5 7 3 5 7 8 10\n",
      "\n",
      "Example #2\n",
      "Src :  8 8 3 6 5 2 8 6 2\n",
      "Trg :  8 8 3 6 5 2 8 6 2\n",
      "Pred:  8 8 3 8 5 2 6 8 2\n",
      "\n",
      "Epoch 3\n",
      "Epoch Step: 50 Loss: 7.275529 Tokens per Sec: 20542.111412\n",
      "Epoch Step: 100 Loss: 6.582399 Tokens per Sec: 19210.008877\n",
      "Evaluation perplexity: 2.072119\n",
      "\n",
      "Example #1\n",
      "Src :  4 8 5 7 10 3 7 8 5\n",
      "Trg :  4 8 5 7 10 3 7 8 5\n",
      "Pred:  4 8 5 7 3 5 8 7 5\n",
      "\n",
      "Example #2\n",
      "Src :  8 8 3 6 5 2 8 6 2\n",
      "Trg :  8 8 3 6 5 2 8 6 2\n",
      "Pred:  8 8 3 6 2 5 8 2 6\n",
      "\n",
      "Epoch 4\n",
      "Epoch Step: 50 Loss: 5.942049 Tokens per Sec: 22952.818446\n",
      "Epoch Step: 100 Loss: 4.906624 Tokens per Sec: 20616.149129\n",
      "Evaluation perplexity: 1.765160\n",
      "\n",
      "Example #1\n",
      "Src :  4 8 5 7 10 3 7 8 5\n",
      "Trg :  4 8 5 7 10 3 7 8 5\n",
      "Pred:  4 8 5 7 3 5 10 8 7\n",
      "\n",
      "Example #2\n",
      "Src :  8 8 3 6 5 2 8 6 2\n",
      "Trg :  8 8 3 6 5 2 8 6 2\n",
      "Pred:  8 8 3 6 5 2 8 6 2\n",
      "\n",
      "Epoch 5\n",
      "Epoch Step: 50 Loss: 5.266299 Tokens per Sec: 18063.724494\n",
      "Epoch Step: 100 Loss: 4.140486 Tokens per Sec: 16706.371703\n",
      "Evaluation perplexity: 1.565404\n",
      "\n",
      "Example #1\n",
      "Src :  4 8 5 7 10 3 7 8 5\n",
      "Trg :  4 8 5 7 10 3 7 8 5\n",
      "Pred:  4 8 5 7 3 10 5 7 8\n",
      "\n",
      "Example #2\n",
      "Src :  8 8 3 6 5 2 8 6 2\n",
      "Trg :  8 8 3 6 5 2 8 6 2\n",
      "Pred:  8 8 3 6 5 2 8 6 2\n",
      "\n",
      "Epoch 6\n",
      "Epoch Step: 50 Loss: 3.958579 Tokens per Sec: 18946.221545\n",
      "Epoch Step: 100 Loss: 3.681249 Tokens per Sec: 19008.718055\n",
      "Evaluation perplexity: 1.455613\n",
      "\n",
      "Example #1\n",
      "Src :  4 8 5 7 10 3 7 8 5\n",
      "Trg :  4 8 5 7 10 3 7 8 5\n",
      "Pred:  4 8 5 7 10 3 5 8 7\n",
      "\n",
      "Example #2\n",
      "Src :  8 8 3 6 5 2 8 6 2\n",
      "Trg :  8 8 3 6 5 2 8 6 2\n",
      "Pred:  8 8 3 6 5 2 8 8 6\n",
      "\n",
      "Epoch 7\n",
      "Epoch Step: 50 Loss: 2.987803 Tokens per Sec: 19241.391531\n",
      "Epoch Step: 100 Loss: 2.790763 Tokens per Sec: 21794.599089\n",
      "Evaluation perplexity: 1.366847\n",
      "\n",
      "Example #1\n",
      "Src :  4 8 5 7 10 3 7 8 5\n",
      "Trg :  4 8 5 7 10 3 7 8 5\n",
      "Pred:  4 8 5 7 10 3 7 8 5\n",
      "\n",
      "Example #2\n",
      "Src :  8 8 3 6 5 2 8 6 2\n",
      "Trg :  8 8 3 6 5 2 8 6 2\n",
      "Pred:  8 8 3 6 5 2 8 6 2\n",
      "\n",
      "Epoch 8\n",
      "Epoch Step: 50 Loss: 2.148365 Tokens per Sec: 22373.913810\n",
      "Epoch Step: 100 Loss: 2.303623 Tokens per Sec: 21987.478949\n",
      "Evaluation perplexity: 1.275108\n",
      "\n",
      "Example #1\n",
      "Src :  4 8 5 7 10 3 7 8 5\n",
      "Trg :  4 8 5 7 10 3 7 8 5\n",
      "Pred:  4 8 5 7 10 3 7 8 5\n",
      "\n",
      "Example #2\n",
      "Src :  8 8 3 6 5 2 8 6 2\n",
      "Trg :  8 8 3 6 5 2 8 6 2\n",
      "Pred:  8 8 3 6 5 2 8 6 2\n",
      "\n",
      "Epoch 9\n",
      "Epoch Step: 50 Loss: 2.088934 Tokens per Sec: 22304.986439\n",
      "Epoch Step: 100 Loss: 2.050290 Tokens per Sec: 22493.929452\n",
      "Evaluation perplexity: 1.195517\n",
      "\n",
      "Example #1\n",
      "Src :  4 8 5 7 10 3 7 8 5\n",
      "Trg :  4 8 5 7 10 3 7 8 5\n",
      "Pred:  4 8 5 7 10 3 7 8 5\n",
      "\n",
      "Example #2\n",
      "Src :  8 8 3 6 5 2 8 6 2\n",
      "Trg :  8 8 3 6 5 2 8 6 2\n",
      "Pred:  8 8 3 6 5 2 8 6 2\n",
      "\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": "<Figure size 432x288 with 1 Axes>"
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "dev_perplexities = train_copy_task()\n",
    "\n",
    "def plot_perplexity(perplexities):\n",
    "    \"\"\"plot perplexities\"\"\"\n",
    "    plt.title(\"Perplexity per Epoch\")\n",
    "    plt.xlabel(\"Epoch\")\n",
    "    plt.ylabel(\"Perplexity\")\n",
    "    plt.plot(perplexities)\n",
    "    \n",
    "plot_perplexity(dev_perplexities)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Hello world!\n"
     ]
    }
   ],
   "source": [
    "import spacy\n",
    "spacy_en = spacy.load('en_core_web_sm')\n",
    "def tokenize_en(text):\n",
    "        print(text)\n",
    "        return [tok.text for tok in spacy_en.tokenizer(text)]\n",
    "out = tokenize_en('Hello world!')\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": "['Hello', 'world', '!']"
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "from __future__ import print_function\n",
    "import os\n",
    "\n",
    "from kaznlp.normalization.ininorm import Normalizer\n",
    "\n",
    "from kaznlp.tokenization.tokrex import TokenizeRex\n",
    "from kaznlp.tokenization.tokhmm import TokenizerHMM\n",
    "\n",
    "from kaznlp.lid.lidnb import LidNB\n",
    "\n",
    "from kaznlp.morphology.analyzers import AnalyzerDD\n",
    "from kaznlp.morphology.taggers import TaggerHMM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "from kaznlp.tokenization.tokhmm import TokenizerHMM\n",
    "mdl = os.path.join('kaznlp', 'tokenization', 'tokhmm.mdl')\n",
    "ininormer = TokenizerHMM(model = mdl)\n",
    "txt = u'Көш жүре түзеледі.Ақсақ қой түстен кейін маңырайды.'\n",
    "array_tok = ininormer.tokenize(txt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": "[['Көш', 'жүре', 'түзеледі', '.'],\n ['Ақсақ', 'қой', 'түстен', 'кейін', 'маңырайды', '.']]"
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "array_tok"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A REAL WORLD EXAMPLE "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting git+git://github.com/pytorch/text\n",
      "  Cloning git://github.com/pytorch/text to /tmp/pip-req-build-e2ltnw1f\n",
      "  Running command git clone -q git://github.com/pytorch/text /tmp/pip-req-build-e2ltnw1f\n",
      "  Running command git submodule update --init --recursive -q\n",
      "Requirement already satisfied: spacy in /home/ghost/anaconda3/lib/python3.8/site-packages (3.1.0)\n",
      "Requirement already satisfied: tqdm in /home/ghost/anaconda3/lib/python3.8/site-packages (from torchtext==0.11.0a0+05cb992) (4.50.2)\n",
      "Requirement already satisfied: requests in /home/ghost/anaconda3/lib/python3.8/site-packages (from torchtext==0.11.0a0+05cb992) (2.24.0)\n",
      "Requirement already satisfied: torch in /home/ghost/anaconda3/lib/python3.8/site-packages (from torchtext==0.11.0a0+05cb992) (1.9.0)\n",
      "Requirement already satisfied: numpy in /home/ghost/anaconda3/lib/python3.8/site-packages (from torchtext==0.11.0a0+05cb992) (1.21.0)\n",
      "Requirement already satisfied: catalogue<2.1.0,>=2.0.4 in /home/ghost/anaconda3/lib/python3.8/site-packages (from spacy) (2.0.4)\n",
      "Requirement already satisfied: srsly<3.0.0,>=2.4.1 in /home/ghost/anaconda3/lib/python3.8/site-packages (from spacy) (2.4.1)\n",
      "Requirement already satisfied: pydantic!=1.8,!=1.8.1,<1.9.0,>=1.7.4 in /home/ghost/anaconda3/lib/python3.8/site-packages (from spacy) (1.8.2)\n",
      "Requirement already satisfied: setuptools in /home/ghost/anaconda3/lib/python3.8/site-packages (from spacy) (57.1.0)\n",
      "Requirement already satisfied: jinja2 in /home/ghost/anaconda3/lib/python3.8/site-packages (from spacy) (2.11.2)\n",
      "Requirement already satisfied: spacy-legacy<3.1.0,>=3.0.7 in /home/ghost/anaconda3/lib/python3.8/site-packages (from spacy) (3.0.8)\n",
      "Requirement already satisfied: wasabi<1.1.0,>=0.8.1 in /home/ghost/anaconda3/lib/python3.8/site-packages (from spacy) (0.8.2)\n",
      "Requirement already satisfied: murmurhash<1.1.0,>=0.28.0 in /home/ghost/anaconda3/lib/python3.8/site-packages (from spacy) (1.0.5)\n",
      "Requirement already satisfied: thinc<8.1.0,>=8.0.7 in /home/ghost/anaconda3/lib/python3.8/site-packages (from spacy) (8.0.7)\n",
      "Requirement already satisfied: packaging>=20.0 in /home/ghost/anaconda3/lib/python3.8/site-packages (from spacy) (20.4)\n",
      "Requirement already satisfied: pathy>=0.3.5 in /home/ghost/anaconda3/lib/python3.8/site-packages (from spacy) (0.6.0)\n",
      "Requirement already satisfied: cymem<2.1.0,>=2.0.2 in /home/ghost/anaconda3/lib/python3.8/site-packages (from spacy) (2.0.5)\n",
      "Requirement already satisfied: blis<0.8.0,>=0.4.0 in /home/ghost/anaconda3/lib/python3.8/site-packages (from spacy) (0.7.4)\n",
      "Requirement already satisfied: preshed<3.1.0,>=3.0.2 in /home/ghost/anaconda3/lib/python3.8/site-packages (from spacy) (3.0.5)\n",
      "Requirement already satisfied: typer<0.4.0,>=0.3.0 in /home/ghost/anaconda3/lib/python3.8/site-packages (from spacy) (0.3.2)\n",
      "Requirement already satisfied: pyparsing>=2.0.2 in /home/ghost/anaconda3/lib/python3.8/site-packages (from packaging>=20.0->spacy) (2.4.7)\n",
      "Requirement already satisfied: six in /home/ghost/anaconda3/lib/python3.8/site-packages (from packaging>=20.0->spacy) (1.15.0)\n",
      "Requirement already satisfied: smart-open<6.0.0,>=5.0.0 in /home/ghost/anaconda3/lib/python3.8/site-packages (from pathy>=0.3.5->spacy) (5.1.0)\n",
      "Requirement already satisfied: typing-extensions>=3.7.4.3 in /home/ghost/anaconda3/lib/python3.8/site-packages (from pydantic!=1.8,!=1.8.1,<1.9.0,>=1.7.4->spacy) (3.7.4.3)\n",
      "Requirement already satisfied: idna<3,>=2.5 in /home/ghost/anaconda3/lib/python3.8/site-packages (from requests->torchtext==0.11.0a0+05cb992) (2.10)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /home/ghost/anaconda3/lib/python3.8/site-packages (from requests->torchtext==0.11.0a0+05cb992) (2020.6.20)\n",
      "Requirement already satisfied: chardet<4,>=3.0.2 in /home/ghost/anaconda3/lib/python3.8/site-packages (from requests->torchtext==0.11.0a0+05cb992) (3.0.4)\n",
      "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /home/ghost/anaconda3/lib/python3.8/site-packages (from requests->torchtext==0.11.0a0+05cb992) (1.25.11)\n",
      "Requirement already satisfied: click<7.2.0,>=7.1.1 in /home/ghost/anaconda3/lib/python3.8/site-packages (from typer<0.4.0,>=0.3.0->spacy) (7.1.2)\n",
      "Requirement already satisfied: MarkupSafe>=0.23 in /home/ghost/anaconda3/lib/python3.8/site-packages (from jinja2->spacy) (1.1.1)\n",
      "\u001b[33mWARNING: You are using pip version 21.1.3; however, version 21.2.1 is available.\n",
      "You should consider upgrading via the '/home/ghost/anaconda3/bin/python -m pip install --upgrade pip' command.\u001b[0m\n",
      "\u001b[38;5;3m⚠ As of spaCy v3.0, shortcuts like 'en' are deprecated. Please use the\n",
      "full pipeline package name 'en_core_web_sm' instead.\u001b[0m\n",
      "Collecting en-core-web-sm==3.1.0\n",
      "  Downloading https://github.com/explosion/spacy-models/releases/download/en_core_web_sm-3.1.0/en_core_web_sm-3.1.0-py3-none-any.whl (13.6 MB)\n",
      "\u001b[K     |████████████████████████████████| 13.6 MB 66 kB/s \n",
      "\u001b[?25hRequirement already satisfied: spacy<3.2.0,>=3.1.0 in /home/ghost/anaconda3/lib/python3.8/site-packages (from en-core-web-sm==3.1.0) (3.1.0)\n",
      "Requirement already satisfied: setuptools in /home/ghost/anaconda3/lib/python3.8/site-packages (from spacy<3.2.0,>=3.1.0->en-core-web-sm==3.1.0) (57.1.0)\n",
      "Requirement already satisfied: cymem<2.1.0,>=2.0.2 in /home/ghost/anaconda3/lib/python3.8/site-packages (from spacy<3.2.0,>=3.1.0->en-core-web-sm==3.1.0) (2.0.5)\n",
      "Requirement already satisfied: typer<0.4.0,>=0.3.0 in /home/ghost/anaconda3/lib/python3.8/site-packages (from spacy<3.2.0,>=3.1.0->en-core-web-sm==3.1.0) (0.3.2)\n",
      "Requirement already satisfied: requests<3.0.0,>=2.13.0 in /home/ghost/anaconda3/lib/python3.8/site-packages (from spacy<3.2.0,>=3.1.0->en-core-web-sm==3.1.0) (2.24.0)\n",
      "Requirement already satisfied: catalogue<2.1.0,>=2.0.4 in /home/ghost/anaconda3/lib/python3.8/site-packages (from spacy<3.2.0,>=3.1.0->en-core-web-sm==3.1.0) (2.0.4)\n",
      "Requirement already satisfied: numpy>=1.15.0 in /home/ghost/anaconda3/lib/python3.8/site-packages (from spacy<3.2.0,>=3.1.0->en-core-web-sm==3.1.0) (1.21.0)\n",
      "Requirement already satisfied: spacy-legacy<3.1.0,>=3.0.7 in /home/ghost/anaconda3/lib/python3.8/site-packages (from spacy<3.2.0,>=3.1.0->en-core-web-sm==3.1.0) (3.0.8)\n",
      "Requirement already satisfied: blis<0.8.0,>=0.4.0 in /home/ghost/anaconda3/lib/python3.8/site-packages (from spacy<3.2.0,>=3.1.0->en-core-web-sm==3.1.0) (0.7.4)\n",
      "Requirement already satisfied: tqdm<5.0.0,>=4.38.0 in /home/ghost/anaconda3/lib/python3.8/site-packages (from spacy<3.2.0,>=3.1.0->en-core-web-sm==3.1.0) (4.50.2)\n",
      "Requirement already satisfied: pydantic!=1.8,!=1.8.1,<1.9.0,>=1.7.4 in /home/ghost/anaconda3/lib/python3.8/site-packages (from spacy<3.2.0,>=3.1.0->en-core-web-sm==3.1.0) (1.8.2)\n",
      "Requirement already satisfied: srsly<3.0.0,>=2.4.1 in /home/ghost/anaconda3/lib/python3.8/site-packages (from spacy<3.2.0,>=3.1.0->en-core-web-sm==3.1.0) (2.4.1)\n",
      "Requirement already satisfied: pathy>=0.3.5 in /home/ghost/anaconda3/lib/python3.8/site-packages (from spacy<3.2.0,>=3.1.0->en-core-web-sm==3.1.0) (0.6.0)\n",
      "Requirement already satisfied: jinja2 in /home/ghost/anaconda3/lib/python3.8/site-packages (from spacy<3.2.0,>=3.1.0->en-core-web-sm==3.1.0) (2.11.2)\n",
      "Requirement already satisfied: thinc<8.1.0,>=8.0.7 in /home/ghost/anaconda3/lib/python3.8/site-packages (from spacy<3.2.0,>=3.1.0->en-core-web-sm==3.1.0) (8.0.7)\n",
      "Requirement already satisfied: wasabi<1.1.0,>=0.8.1 in /home/ghost/anaconda3/lib/python3.8/site-packages (from spacy<3.2.0,>=3.1.0->en-core-web-sm==3.1.0) (0.8.2)\n",
      "Requirement already satisfied: preshed<3.1.0,>=3.0.2 in /home/ghost/anaconda3/lib/python3.8/site-packages (from spacy<3.2.0,>=3.1.0->en-core-web-sm==3.1.0) (3.0.5)\n",
      "Requirement already satisfied: murmurhash<1.1.0,>=0.28.0 in /home/ghost/anaconda3/lib/python3.8/site-packages (from spacy<3.2.0,>=3.1.0->en-core-web-sm==3.1.0) (1.0.5)\n",
      "Requirement already satisfied: packaging>=20.0 in /home/ghost/anaconda3/lib/python3.8/site-packages (from spacy<3.2.0,>=3.1.0->en-core-web-sm==3.1.0) (20.4)\n",
      "Requirement already satisfied: six in /home/ghost/anaconda3/lib/python3.8/site-packages (from packaging>=20.0->spacy<3.2.0,>=3.1.0->en-core-web-sm==3.1.0) (1.15.0)\n",
      "Requirement already satisfied: pyparsing>=2.0.2 in /home/ghost/anaconda3/lib/python3.8/site-packages (from packaging>=20.0->spacy<3.2.0,>=3.1.0->en-core-web-sm==3.1.0) (2.4.7)\n",
      "Requirement already satisfied: smart-open<6.0.0,>=5.0.0 in /home/ghost/anaconda3/lib/python3.8/site-packages (from pathy>=0.3.5->spacy<3.2.0,>=3.1.0->en-core-web-sm==3.1.0) (5.1.0)\n",
      "Requirement already satisfied: typing-extensions>=3.7.4.3 in /home/ghost/anaconda3/lib/python3.8/site-packages (from pydantic!=1.8,!=1.8.1,<1.9.0,>=1.7.4->spacy<3.2.0,>=3.1.0->en-core-web-sm==3.1.0) (3.7.4.3)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /home/ghost/anaconda3/lib/python3.8/site-packages (from requests<3.0.0,>=2.13.0->spacy<3.2.0,>=3.1.0->en-core-web-sm==3.1.0) (2020.6.20)\n",
      "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /home/ghost/anaconda3/lib/python3.8/site-packages (from requests<3.0.0,>=2.13.0->spacy<3.2.0,>=3.1.0->en-core-web-sm==3.1.0) (1.25.11)\n",
      "Requirement already satisfied: idna<3,>=2.5 in /home/ghost/anaconda3/lib/python3.8/site-packages (from requests<3.0.0,>=2.13.0->spacy<3.2.0,>=3.1.0->en-core-web-sm==3.1.0) (2.10)\n",
      "Requirement already satisfied: chardet<4,>=3.0.2 in /home/ghost/anaconda3/lib/python3.8/site-packages (from requests<3.0.0,>=2.13.0->spacy<3.2.0,>=3.1.0->en-core-web-sm==3.1.0) (3.0.4)\n",
      "Requirement already satisfied: click<7.2.0,>=7.1.1 in /home/ghost/anaconda3/lib/python3.8/site-packages (from typer<0.4.0,>=0.3.0->spacy<3.2.0,>=3.1.0->en-core-web-sm==3.1.0) (7.1.2)\n",
      "Requirement already satisfied: MarkupSafe>=0.23 in /home/ghost/anaconda3/lib/python3.8/site-packages (from jinja2->spacy<3.2.0,>=3.1.0->en-core-web-sm==3.1.0) (1.1.1)\n",
      "\u001b[33mWARNING: You are using pip version 21.1.3; however, version 21.2.1 is available.\n",
      "You should consider upgrading via the '/home/ghost/anaconda3/bin/python3 -m pip install --upgrade pip' command.\u001b[0m\n",
      "\u001b[38;5;2m✔ Download and installation successful\u001b[0m\n",
      "You can now load the package via spacy.load('en_core_web_sm')\n",
      "\u001b[38;5;3m⚠ As of spaCy v3.0, shortcuts like 'de' are deprecated. Please use the\n",
      "full pipeline package name 'de_core_news_sm' instead.\u001b[0m\n",
      "Collecting de-core-news-sm==3.1.0\n",
      "  Downloading https://github.com/explosion/spacy-models/releases/download/de_core_news_sm-3.1.0/de_core_news_sm-3.1.0-py3-none-any.whl (18.8 MB)\n",
      "\u001b[K     |████████████████████████████████| 18.8 MB 137 kB/s \n",
      "\u001b[?25hRequirement already satisfied: spacy<3.2.0,>=3.1.0 in /home/ghost/anaconda3/lib/python3.8/site-packages (from de-core-news-sm==3.1.0) (3.1.0)\n",
      "Requirement already satisfied: catalogue<2.1.0,>=2.0.4 in /home/ghost/anaconda3/lib/python3.8/site-packages (from spacy<3.2.0,>=3.1.0->de-core-news-sm==3.1.0) (2.0.4)\n",
      "Requirement already satisfied: setuptools in /home/ghost/anaconda3/lib/python3.8/site-packages (from spacy<3.2.0,>=3.1.0->de-core-news-sm==3.1.0) (57.1.0)\n",
      "Requirement already satisfied: typer<0.4.0,>=0.3.0 in /home/ghost/anaconda3/lib/python3.8/site-packages (from spacy<3.2.0,>=3.1.0->de-core-news-sm==3.1.0) (0.3.2)\n",
      "Requirement already satisfied: jinja2 in /home/ghost/anaconda3/lib/python3.8/site-packages (from spacy<3.2.0,>=3.1.0->de-core-news-sm==3.1.0) (2.11.2)\n",
      "Requirement already satisfied: numpy>=1.15.0 in /home/ghost/anaconda3/lib/python3.8/site-packages (from spacy<3.2.0,>=3.1.0->de-core-news-sm==3.1.0) (1.21.0)\n",
      "Requirement already satisfied: srsly<3.0.0,>=2.4.1 in /home/ghost/anaconda3/lib/python3.8/site-packages (from spacy<3.2.0,>=3.1.0->de-core-news-sm==3.1.0) (2.4.1)\n",
      "Requirement already satisfied: preshed<3.1.0,>=3.0.2 in /home/ghost/anaconda3/lib/python3.8/site-packages (from spacy<3.2.0,>=3.1.0->de-core-news-sm==3.1.0) (3.0.5)\n",
      "Requirement already satisfied: requests<3.0.0,>=2.13.0 in /home/ghost/anaconda3/lib/python3.8/site-packages (from spacy<3.2.0,>=3.1.0->de-core-news-sm==3.1.0) (2.24.0)\n",
      "Requirement already satisfied: spacy-legacy<3.1.0,>=3.0.7 in /home/ghost/anaconda3/lib/python3.8/site-packages (from spacy<3.2.0,>=3.1.0->de-core-news-sm==3.1.0) (3.0.8)\n",
      "Requirement already satisfied: wasabi<1.1.0,>=0.8.1 in /home/ghost/anaconda3/lib/python3.8/site-packages (from spacy<3.2.0,>=3.1.0->de-core-news-sm==3.1.0) (0.8.2)\n",
      "Requirement already satisfied: thinc<8.1.0,>=8.0.7 in /home/ghost/anaconda3/lib/python3.8/site-packages (from spacy<3.2.0,>=3.1.0->de-core-news-sm==3.1.0) (8.0.7)\n",
      "Requirement already satisfied: tqdm<5.0.0,>=4.38.0 in /home/ghost/anaconda3/lib/python3.8/site-packages (from spacy<3.2.0,>=3.1.0->de-core-news-sm==3.1.0) (4.50.2)\n",
      "Requirement already satisfied: pathy>=0.3.5 in /home/ghost/anaconda3/lib/python3.8/site-packages (from spacy<3.2.0,>=3.1.0->de-core-news-sm==3.1.0) (0.6.0)\n",
      "Requirement already satisfied: murmurhash<1.1.0,>=0.28.0 in /home/ghost/anaconda3/lib/python3.8/site-packages (from spacy<3.2.0,>=3.1.0->de-core-news-sm==3.1.0) (1.0.5)\n",
      "Requirement already satisfied: cymem<2.1.0,>=2.0.2 in /home/ghost/anaconda3/lib/python3.8/site-packages (from spacy<3.2.0,>=3.1.0->de-core-news-sm==3.1.0) (2.0.5)\n",
      "Requirement already satisfied: pydantic!=1.8,!=1.8.1,<1.9.0,>=1.7.4 in /home/ghost/anaconda3/lib/python3.8/site-packages (from spacy<3.2.0,>=3.1.0->de-core-news-sm==3.1.0) (1.8.2)\n",
      "Requirement already satisfied: packaging>=20.0 in /home/ghost/anaconda3/lib/python3.8/site-packages (from spacy<3.2.0,>=3.1.0->de-core-news-sm==3.1.0) (20.4)\n",
      "Requirement already satisfied: blis<0.8.0,>=0.4.0 in /home/ghost/anaconda3/lib/python3.8/site-packages (from spacy<3.2.0,>=3.1.0->de-core-news-sm==3.1.0) (0.7.4)\n",
      "Requirement already satisfied: pyparsing>=2.0.2 in /home/ghost/anaconda3/lib/python3.8/site-packages (from packaging>=20.0->spacy<3.2.0,>=3.1.0->de-core-news-sm==3.1.0) (2.4.7)\n",
      "Requirement already satisfied: six in /home/ghost/anaconda3/lib/python3.8/site-packages (from packaging>=20.0->spacy<3.2.0,>=3.1.0->de-core-news-sm==3.1.0) (1.15.0)\n",
      "Requirement already satisfied: smart-open<6.0.0,>=5.0.0 in /home/ghost/anaconda3/lib/python3.8/site-packages (from pathy>=0.3.5->spacy<3.2.0,>=3.1.0->de-core-news-sm==3.1.0) (5.1.0)\n",
      "Requirement already satisfied: typing-extensions>=3.7.4.3 in /home/ghost/anaconda3/lib/python3.8/site-packages (from pydantic!=1.8,!=1.8.1,<1.9.0,>=1.7.4->spacy<3.2.0,>=3.1.0->de-core-news-sm==3.1.0) (3.7.4.3)\n",
      "Requirement already satisfied: idna<3,>=2.5 in /home/ghost/anaconda3/lib/python3.8/site-packages (from requests<3.0.0,>=2.13.0->spacy<3.2.0,>=3.1.0->de-core-news-sm==3.1.0) (2.10)\n",
      "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /home/ghost/anaconda3/lib/python3.8/site-packages (from requests<3.0.0,>=2.13.0->spacy<3.2.0,>=3.1.0->de-core-news-sm==3.1.0) (1.25.11)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /home/ghost/anaconda3/lib/python3.8/site-packages (from requests<3.0.0,>=2.13.0->spacy<3.2.0,>=3.1.0->de-core-news-sm==3.1.0) (2020.6.20)\n",
      "Requirement already satisfied: chardet<4,>=3.0.2 in /home/ghost/anaconda3/lib/python3.8/site-packages (from requests<3.0.0,>=2.13.0->spacy<3.2.0,>=3.1.0->de-core-news-sm==3.1.0) (3.0.4)\n",
      "Requirement already satisfied: click<7.2.0,>=7.1.1 in /home/ghost/anaconda3/lib/python3.8/site-packages (from typer<0.4.0,>=0.3.0->spacy<3.2.0,>=3.1.0->de-core-news-sm==3.1.0) (7.1.2)\n",
      "Requirement already satisfied: MarkupSafe>=0.23 in /home/ghost/anaconda3/lib/python3.8/site-packages (from jinja2->spacy<3.2.0,>=3.1.0->de-core-news-sm==3.1.0) (1.1.1)\n",
      "\u001b[33mWARNING: You are using pip version 21.1.3; however, version 21.2.1 is available.\n",
      "You should consider upgrading via the '/home/ghost/anaconda3/bin/python3 -m pip install --upgrade pip' command.\u001b[0m\n",
      "\u001b[38;5;2m✔ Download and installation successful\u001b[0m\n",
      "You can now load the package via spacy.load('de_core_news_sm')\n"
     ]
    }
   ],
   "source": [
    "!pip3 install git+git://github.com/pytorch/text spacy \n",
    "!python3 -m spacy download en\n",
    "!python3 -m spacy download de"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "!export LC_ALL=\"en_US.UTF-8\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'torchtext'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "\u001b[0;32m/tmp/ipykernel_13946/2591755545.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0mtorchtext\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mdata\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdatasets\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mtorchtext\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlegacy\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mdata\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mtorchtext\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlegacy\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mdatasets\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'torchtext'"
     ]
    }
   ],
   "source": [
    "from torchtext import data, datasets\n",
    "from torchtext.legacy import data\n",
    "from torchtext.legacy import datasets\n",
    "\n",
    "if True:\n",
    "    import spacy\n",
    "    spacy_de = spacy.load(\"de_core_news_sm\")\n",
    "    spacy_en = spacy.load(\"en_core_web_sm\")\n",
    "\n",
    "    def tokenize_de(text):\n",
    "        return [tok.text for tok in spacy_de.tokenizer(text)]\n",
    "\n",
    "    def tokenize_en(text):\n",
    "        return [tok.text for tok in spacy_en.tokenizer(text)]\n",
    "\n",
    "    print(\"KEk\")\n",
    "\n",
    "    UNK_TOKEN = \"<unk>\"\n",
    "    PAD_TOKEN = \"<pad>\"    \n",
    "    SOS_TOKEN = \"<s>\"\n",
    "    EOS_TOKEN = \"</s>\"\n",
    "    LOWER = True\n",
    "    \n",
    "    # we include lengths to provide to the RNNs\n",
    "    SRC = data.Field(tokenize=tokenize_de, \n",
    "                     batch_first=True, lower=LOWER, include_lengths=True,\n",
    "                     unk_token=UNK_TOKEN, pad_token=PAD_TOKEN, init_token=None, eos_token=EOS_TOKEN)\n",
    "    TRG = data.Field(tokenize=tokenize_en, \n",
    "                     batch_first=True, lower=LOWER, include_lengths=True,\n",
    "                     unk_token=UNK_TOKEN, pad_token=PAD_TOKEN, init_token=SOS_TOKEN, eos_token=EOS_TOKEN)\n",
    "\n",
    "    print(\"KEK\")\n",
    "    MAX_LEN = 25  # NOTE: we filter out a lot of sentences for speed\n",
    "    print(\"KEK\")\n",
    "    train_data, valid_data, test_data = datasets.IWSLT.splits(\n",
    "        exts=('.de', '.en'), fields=(SRC, TRG), \n",
    "        filter_pred=lambda x: len(vars(x)['src']) <= MAX_LEN and \n",
    "            len(vars(x)['trg']) <= MAX_LEN) \n",
    "    print(\"KEK\")\n",
    "    MIN_FREQ = 5  # NOTE: we limit the vocabulary to frequent words for speed\n",
    "    SRC.build_vocab(train_data.src, min_freq=MIN_FREQ)\n",
    "    TRG.build_vocab(train_data.trg, min_freq=MIN_FREQ)\n",
    "    print(\"KEK\")\n",
    "    \n",
    "    PAD_INDEX = TRG.vocab.stoi[PAD_TOKEN]\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "ename": "Error",
     "evalue": "Session cannot generate requests",
     "output_type": "error",
     "traceback": [
      "Error: Session cannot generate requests",
      "at w.executeCodeCell (/home/ghost/.vscode/extensions/ms-toolsai.jupyter-2021.6.832593372/out/client/extension.js:90:316184)",
      "at w.execute (/home/ghost/.vscode/extensions/ms-toolsai.jupyter-2021.6.832593372/out/client/extension.js:90:315573)",
      "at w.start (/home/ghost/.vscode/extensions/ms-toolsai.jupyter-2021.6.832593372/out/client/extension.js:90:311378)",
      "at runMicrotasks (<anonymous>)",
      "at processTicksAndRejections (internal/process/task_queues.js:93:5)",
      "at async t.CellExecutionQueue.executeQueuedCells (/home/ghost/.vscode/extensions/ms-toolsai.jupyter-2021.6.832593372/out/client/extension.js:90:325786)",
      "at async t.CellExecutionQueue.start (/home/ghost/.vscode/extensions/ms-toolsai.jupyter-2021.6.832593372/out/client/extension.js:90:325326)"
     ]
    }
   ],
   "source": [
    "from torchtext import data, datasets\n",
    "\n",
    "if True:\n",
    "    import spacy\n",
    "    spacy_de = spacy.load('de')\n",
    "    spacy_en = spacy.load('en')\n",
    "\n",
    "    def tokenize_de(text):\n",
    "        return [tok.text for tok in spacy_de.tokenizer(text)]\n",
    "\n",
    "    def tokenize_en(text):\n",
    "        return [tok.text for tok in spacy_en.tokenizer(text)]\n",
    "\n",
    "    UNK_TOKEN = \"<unk>\"\n",
    "    PAD_TOKEN = \"<pad>\"    \n",
    "    SOS_TOKEN = \"<s>\"\n",
    "    EOS_TOKEN = \"</s>\"\n",
    "    LOWER = True\n",
    "    \n",
    "    # we include lengths to provide to the RNNs\n",
    "    SRC = data.Field(tokenize=tokenize_de, \n",
    "                     batch_first=True, lower=LOWER, include_lengths=True,\n",
    "                     unk_token=UNK_TOKEN, pad_token=PAD_TOKEN, init_token=None, eos_token=EOS_TOKEN)\n",
    "    TRG = data.Field(tokenize=tokenize_en, \n",
    "                     batch_first=True, lower=LOWER, include_lengths=True,\n",
    "                     unk_token=UNK_TOKEN, pad_token=PAD_TOKEN, init_token=SOS_TOKEN, eos_token=EOS_TOKEN)\n",
    "\n",
    "    MAX_LEN = 25  # NOTE: we filter out a lot of sentences for speed\n",
    "    train_data, valid_data, test_data = datasets.IWSLT.splits(\n",
    "        exts=('.de', '.en'), fields=(SRC, TRG), \n",
    "        filter_pred=lambda x: len(vars(x)['src']) <= MAX_LEN and \n",
    "            len(vars(x)['trg']) <= MAX_LEN)\n",
    "    MIN_FREQ = 5  # NOTE: we limit the vocabulary to frequent words for speed\n",
    "    SRC.build_vocab(train_data.src, min_freq=MIN_FREQ)\n",
    "    TRG.build_vocab(train_data.trg, min_freq=MIN_FREQ)\n",
    "    \n",
    "    PAD_INDEX = TRG.vocab.stoi[PAD_TOKEN]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "df = pd.read_csv('out_final.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>samples</th>\n      <th>predicted</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>Әлеуметтік желілерде ЕЦ-166/5 төтенше қауіпсіз...</td>\n      <td>Әлеуметтік желілерде ЕЦ жүз алпыс алты бөлу бе...</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>Бұл – фейк, оны Нұр-Сұлтан қаласы бойынша ҚАЖД...</td>\n      <td>Бұл   фейк, оны Нұр Сұлтан қаласы бойынша ҚАЖД...</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>Кейбір желі қолданушылары мекеменің алдына қой...</td>\n      <td>Кейбір желі қолданушылары мекеменің алдына қой...</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>Шын мәнінде дәрігерлер колонияға созылмалы аур...</td>\n      <td>Шын мәнінде дәрігерлер колонияға созылмалы аур...</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>«17 наурызда таңғы сағат 06:30 шамасында Нұр-С...</td>\n      <td>«он жетінші наурызда таңғы сағат алты отыз шам...</td>\n    </tr>\n  </tbody>\n</table>\n</div>",
      "text/plain": "                                             samples  \\\n0  Әлеуметтік желілерде ЕЦ-166/5 төтенше қауіпсіз...   \n1  Бұл – фейк, оны Нұр-Сұлтан қаласы бойынша ҚАЖД...   \n2  Кейбір желі қолданушылары мекеменің алдына қой...   \n3  Шын мәнінде дәрігерлер колонияға созылмалы аур...   \n4  «17 наурызда таңғы сағат 06:30 шамасында Нұр-С...   \n\n                                           predicted  \n0  Әлеуметтік желілерде ЕЦ жүз алпыс алты бөлу бе...  \n1  Бұл   фейк, оны Нұр Сұлтан қаласы бойынша ҚАЖД...  \n2  Кейбір желі қолданушылары мекеменің алдына қой...  \n3  Шын мәнінде дәрігерлер колонияға созылмалы аур...  \n4  «он жетінші наурызда таңғы сағат алты отыз шам...  "
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "an integer is required (got type bytes)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-42-e6700460c03f>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mpandas\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mpd\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0msklearn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmodel_selection\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mtrain_test_split\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      3\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0mtrain_size\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m0.8\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.8/site-packages/sklearn/__init__.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     62\u001b[0m \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     63\u001b[0m     \u001b[0;32mfrom\u001b[0m \u001b[0;34m.\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0m__check_build\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 64\u001b[0;31m     \u001b[0;32mfrom\u001b[0m \u001b[0;34m.\u001b[0m\u001b[0mbase\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mclone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     65\u001b[0m     \u001b[0;32mfrom\u001b[0m \u001b[0;34m.\u001b[0m\u001b[0mutils\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_show_versions\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mshow_versions\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     66\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.8/site-packages/sklearn/base.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     11\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mscipy\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0msparse\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     12\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0;34m.\u001b[0m\u001b[0mexternals\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0msix\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 13\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0;34m.\u001b[0m\u001b[0mutils\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfixes\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0msignature\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     14\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0;34m.\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0m__version__\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     15\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.8/site-packages/sklearn/utils/__init__.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     12\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0;34m.\u001b[0m\u001b[0mmurmurhash\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mmurmurhash3_32\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     13\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0;34m.\u001b[0m\u001b[0mclass_weight\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mcompute_class_weight\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcompute_sample_weight\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 14\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0;34m.\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0m_joblib\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     15\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0;34m.\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mexceptions\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mDataConversionWarning\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     16\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0;34m.\u001b[0m\u001b[0mfixes\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0m_Sequence\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mSequence\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.8/site-packages/sklearn/utils/_joblib.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     20\u001b[0m         \u001b[0;32mfrom\u001b[0m \u001b[0mjoblib\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mparallel_backend\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mregister_parallel_backend\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     21\u001b[0m \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 22\u001b[0;31m     \u001b[0;32mfrom\u001b[0m \u001b[0;34m.\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mexternals\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mjoblib\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     23\u001b[0m     \u001b[0;32mfrom\u001b[0m \u001b[0;34m.\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mexternals\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mjoblib\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mlogger\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     24\u001b[0m     \u001b[0;32mfrom\u001b[0m \u001b[0;34m.\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mexternals\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mjoblib\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mdump\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mload\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.8/site-packages/sklearn/externals/joblib/__init__.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m    117\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0;34m.\u001b[0m\u001b[0mnumpy_pickle\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mload\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    118\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0;34m.\u001b[0m\u001b[0mcompressor\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mregister_compressor\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 119\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0;34m.\u001b[0m\u001b[0mparallel\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mParallel\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    120\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0;34m.\u001b[0m\u001b[0mparallel\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mdelayed\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    121\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0;34m.\u001b[0m\u001b[0mparallel\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mcpu_count\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.8/site-packages/sklearn/externals/joblib/parallel.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     26\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0;34m.\u001b[0m\u001b[0mmy_exceptions\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mTransportableException\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     27\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0;34m.\u001b[0m\u001b[0mdisk\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mmemstr_to_bytes\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 28\u001b[0;31m from ._parallel_backends import (FallbackToBackend, MultiprocessingBackend,\n\u001b[0m\u001b[1;32m     29\u001b[0m                                  \u001b[0mThreadingBackend\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mSequentialBackend\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     30\u001b[0m                                  LokyBackend)\n",
      "\u001b[0;32m~/anaconda3/lib/python3.8/site-packages/sklearn/externals/joblib/_parallel_backends.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     20\u001b[0m     \u001b[0;32mfrom\u001b[0m \u001b[0;34m.\u001b[0m\u001b[0mpool\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mMemmappingPool\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     21\u001b[0m     \u001b[0;32mfrom\u001b[0m \u001b[0mmultiprocessing\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpool\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mThreadPool\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 22\u001b[0;31m     \u001b[0;32mfrom\u001b[0m \u001b[0;34m.\u001b[0m\u001b[0mexecutor\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mget_memmapping_executor\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     23\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     24\u001b[0m     \u001b[0;31m# Compat between concurrent.futures and multiprocessing TimeoutError\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.8/site-packages/sklearn/externals/joblib/executor.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     12\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0;34m.\u001b[0m\u001b[0mdisk\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mdelete_folder\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     13\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0;34m.\u001b[0m\u001b[0m_memmapping_reducer\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mget_memmapping_reducers\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 14\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0;34m.\u001b[0m\u001b[0mexternals\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mloky\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreusable_executor\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mget_reusable_executor\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     15\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     16\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.8/site-packages/sklearn/externals/joblib/externals/loky/__init__.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     10\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     11\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0;34m.\u001b[0m\u001b[0mbackend\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcontext\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mcpu_count\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 12\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0;34m.\u001b[0m\u001b[0mbackend\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreduction\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mset_loky_pickler\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     13\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0;34m.\u001b[0m\u001b[0mreusable_executor\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mget_reusable_executor\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     14\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0;34m.\u001b[0m\u001b[0mcloudpickle_wrapper\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mwrap_non_picklable_objects\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.8/site-packages/sklearn/externals/joblib/externals/loky/backend/reduction.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m    123\u001b[0m \u001b[0;31m# global variable to change the pickler behavior\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    124\u001b[0m \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 125\u001b[0;31m     \u001b[0;32mfrom\u001b[0m \u001b[0msklearn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mexternals\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mjoblib\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mexternals\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mcloudpickle\u001b[0m  \u001b[0;31m# noqa: F401\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    126\u001b[0m     \u001b[0mDEFAULT_ENV\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m\"cloudpickle\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    127\u001b[0m \u001b[0;32mexcept\u001b[0m \u001b[0mImportError\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.8/site-packages/sklearn/externals/joblib/externals/cloudpickle/__init__.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0m__future__\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mabsolute_import\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0;34m.\u001b[0m\u001b[0mcloudpickle\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      4\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0m__version__\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m'0.6.1'\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.8/site-packages/sklearn/externals/joblib/externals/cloudpickle/cloudpickle.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m    165\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    166\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 167\u001b[0;31m \u001b[0m_cell_set_template_code\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_make_cell_set_template_code\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    168\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    169\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.8/site-packages/sklearn/externals/joblib/externals/cloudpickle/cloudpickle.py\u001b[0m in \u001b[0;36m_make_cell_set_template_code\u001b[0;34m()\u001b[0m\n\u001b[1;32m    146\u001b[0m         )\n\u001b[1;32m    147\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 148\u001b[0;31m         return types.CodeType(\n\u001b[0m\u001b[1;32m    149\u001b[0m             \u001b[0mco\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mco_argcount\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    150\u001b[0m             \u001b[0mco\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mco_kwonlyargcount\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mTypeError\u001b[0m: an integer is required (got type bytes)"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "train_size = 0.8\n",
    "\n",
    "X = df.drop(columns = [\"predicted\"]).copy()\n",
    "y = df[\"predicted\"]\n",
    "\n",
    "X_train, X_rem, y_train, y_rem = train_test_split(X, y, train_size=0.8)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'train_test_split' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-18-0638f0c601ff>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0mtest_size\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m0.5\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0mX_valid\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mX_test\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_valid\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_test\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtrain_test_split\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX_rem\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_rem\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtest_size\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m0.5\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m: name 'train_test_split' is not defined"
     ]
    }
   ],
   "source": [
    "test_size=0.5\n",
    "X_valid, X_test, y_valid, y_test = train_test_split(X_rem, y_rem, test_size=0.5)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(3001781, 1)\n",
      "(3001781,)\n",
      "(375223, 1)\n",
      "(375223,)\n",
      "(375223, 1)\n",
      "(375223,)\n"
     ]
    },
    {
     "data": {
      "text/plain": "(None, None)"
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print(X_train.shape), print(y_train.shape)\n",
    "print(X_valid.shape), print(y_valid.shape)\n",
    "print(X_test.shape), print(y_test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'y_train' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-27-f56205002e60>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0my_train\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m: name 'y_train' is not defined"
     ]
    }
   ],
   "source": [
    "y_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>samples</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>681727</th>\n      <td>Сонымен қатар, Қылмыстық кодекстің 297-бабы бо...</td>\n    </tr>\n    <tr>\n      <th>1025183</th>\n      <td>Баспасөз қызметінің мәліметінше, су тасқынынан...</td>\n    </tr>\n    <tr>\n      <th>2698063</th>\n      <td>Б кезеңінде жаһандық жүйе пайда болды (1750 — ...</td>\n    </tr>\n    <tr>\n      <th>1188840</th>\n      <td>Ол қазіргі Атырау облысының Манаш ауылында дүн...</td>\n    </tr>\n    <tr>\n      <th>2725731</th>\n      <td>Қосымша жіптен 156 шалу теріп алып, бірнеше қа...</td>\n    </tr>\n    <tr>\n      <th>...</th>\n      <td>...</td>\n    </tr>\n    <tr>\n      <th>2826766</th>\n      <td>Осының бәрі жергілікті халықтың кедейленіп, са...</td>\n    </tr>\n    <tr>\n      <th>3024945</th>\n      <td>Боланден () — Германия Федеративтік Республика...</td>\n    </tr>\n    <tr>\n      <th>3711233</th>\n      <td>И.\\n</td>\n    </tr>\n    <tr>\n      <th>73991</th>\n      <td>Қарағанды облысының Шахтинск қаласында тұратын...</td>\n    </tr>\n    <tr>\n      <th>1809507</th>\n      <td>Егер өсімдікті қараңғы жерде өсірсе, ол бозары...</td>\n    </tr>\n  </tbody>\n</table>\n<p>3001781 rows × 1 columns</p>\n</div>",
      "text/plain": "                                                   samples\n681727   Сонымен қатар, Қылмыстық кодекстің 297-бабы бо...\n1025183  Баспасөз қызметінің мәліметінше, су тасқынынан...\n2698063  Б кезеңінде жаһандық жүйе пайда болды (1750 — ...\n1188840  Ол қазіргі Атырау облысының Манаш ауылында дүн...\n2725731  Қосымша жіптен 156 шалу теріп алып, бірнеше қа...\n...                                                    ...\n2826766  Осының бәрі жергілікті халықтың кедейленіп, са...\n3024945  Боланден () — Германия Федеративтік Республика...\n3711233                                               И.\\n\n73991    Қарағанды облысының Шахтинск қаласында тұратын...\n1809507  Егер өсімдікті қараңғы жерде өсірсе, ол бозары...\n\n[3001781 rows x 1 columns]"
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>samples</th>\n      <th>predicted</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>Әлеуметтік желілерде ЕЦ-166/5 төтенше қауіпсіз...</td>\n      <td>Әлеуметтік желілерде ЕЦ жүз алпыс алты бөлу бе...</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>Бұл – фейк, оны Нұр-Сұлтан қаласы бойынша ҚАЖД...</td>\n      <td>Бұл   фейк, оны Нұр Сұлтан қаласы бойынша ҚАЖД...</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>Кейбір желі қолданушылары мекеменің алдына қой...</td>\n      <td>Кейбір желі қолданушылары мекеменің алдына қой...</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>Шын мәнінде дәрігерлер колонияға созылмалы аур...</td>\n      <td>Шын мәнінде дәрігерлер колонияға созылмалы аур...</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>«17 наурызда таңғы сағат 06:30 шамасында Нұр-С...</td>\n      <td>«он жетінші наурызда таңғы сағат алты отыз шам...</td>\n    </tr>\n    <tr>\n      <th>...</th>\n      <td>...</td>\n      <td>...</td>\n    </tr>\n    <tr>\n      <th>3752222</th>\n      <td>Қазақтар шекара шебіндегі бекіністерге жүйелі ...</td>\n      <td>Қазақтар шекара шебіндегі бекіністерге жүйелі ...</td>\n    </tr>\n    <tr>\n      <th>3752223</th>\n      <td>Көтеріліске тағы да Жоламан батыр басшылық етт...</td>\n      <td>Көтеріліске тағы да Жоламан батыр басшылық етт...</td>\n    </tr>\n    <tr>\n      <th>3752224</th>\n      <td>Жоламан батыр бастаған қозғалыстың отаршылдыққ...</td>\n      <td>Жоламан батыр бастаған қозғалыстың отаршылдыққ...</td>\n    </tr>\n    <tr>\n      <th>3752225</th>\n      <td>Көтерілістің әр жерде шашыраңқы жүруі мен оған...</td>\n      <td>Көтерілістің әр жерде шашыраңқы жүруі мен оған...</td>\n    </tr>\n    <tr>\n      <th>3752226</th>\n      <td>Солай бола тұрса да ол Кенесары Қасымұлы баста...</td>\n      <td>Солай бола тұрса да ол Кенесары Қасымұлы баста...</td>\n    </tr>\n  </tbody>\n</table>\n<p>3752227 rows × 2 columns</p>\n</div>",
      "text/plain": "                                                   samples  \\\n0        Әлеуметтік желілерде ЕЦ-166/5 төтенше қауіпсіз...   \n1        Бұл – фейк, оны Нұр-Сұлтан қаласы бойынша ҚАЖД...   \n2        Кейбір желі қолданушылары мекеменің алдына қой...   \n3        Шын мәнінде дәрігерлер колонияға созылмалы аур...   \n4        «17 наурызда таңғы сағат 06:30 шамасында Нұр-С...   \n...                                                    ...   \n3752222  Қазақтар шекара шебіндегі бекіністерге жүйелі ...   \n3752223  Көтеріліске тағы да Жоламан батыр басшылық етт...   \n3752224  Жоламан батыр бастаған қозғалыстың отаршылдыққ...   \n3752225  Көтерілістің әр жерде шашыраңқы жүруі мен оған...   \n3752226  Солай бола тұрса да ол Кенесары Қасымұлы баста...   \n\n                                                 predicted  \n0        Әлеуметтік желілерде ЕЦ жүз алпыс алты бөлу бе...  \n1        Бұл   фейк, оны Нұр Сұлтан қаласы бойынша ҚАЖД...  \n2        Кейбір желі қолданушылары мекеменің алдына қой...  \n3        Шын мәнінде дәрігерлер колонияға созылмалы аур...  \n4        «он жетінші наурызда таңғы сағат алты отыз шам...  \n...                                                    ...  \n3752222  Қазақтар шекара шебіндегі бекіністерге жүйелі ...  \n3752223  Көтеріліске тағы да Жоламан батыр басшылық етт...  \n3752224  Жоламан батыр бастаған қозғалыстың отаршылдыққ...  \n3752225  Көтерілістің әр жерде шашыраңқы жүруі мен оған...  \n3752226  Солай бола тұрса да ол Кенесары Қасымұлы баста...  \n\n[3752227 rows x 2 columns]"
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def batch(iterable, n=1):\n",
    "    l = len(iterable)\n",
    "    for ndx in range(0, l, n):\n",
    "        yield iterable[ndx:min(ndx + n, l)]\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": "'def print_data_info(train_data, valid_data, test_data, src_field, trg_field):\\n    \"\"\" This prints some useful stuff about our data sets. \"\"\"\\n\\n    print(\"Data set sizes (number of sentence pairs):\")\\n    print(\\'train\\', len(train_data))\\n    print(\\'valid\\', len(valid_data))\\n    print(\\'test\\', len(test_data), \"\\n\")\\n\\n    print(\"First training example:\")\\n    print(\"src:\", \" \".join(vars(train_data[0])[\\'samples\\']))\\n    #print(\"trg:\", \" \".join(vars(src_field[0])[1]), \"\\n\")\\n\\n    #print(\"Most common words (src):\")\\n    #print(\"\\n\".join([\"%10s %10d\" % x for x in src_field.vocab.freqs.most_common(10)]), \"\\n\")\\n    #print(\"Most common words (trg):\")\\n    #print(\"\\n\".join([\"%10s %10d\" % x for x in trg_field.vocab.freqs.most_common(10)]), \"\\n\")\\n\\n    #print(\"First 10 words (src):\")\\n    #print(\"\\n\".join(\\n    #    \\'%02d %s\\' % (i, t) for i, t in enumerate(src_field.vocab.itos[:10])), \"\\n\")\\n    #print(\"First 10 words (trg):\")\\n    #print(\"\\n\".join(\\n    #    \\'%02d %s\\' % (i, t) for i, t in enumerate(trg_field.vocab.itos[:10])), \"\\n\")\\n\\n    print(\"Number of German words (types):\", len(src_field.vocab))\\n    print(\"Number of English words (types):\", len(trg_field.vocab), \"\\n\")\\n    \\n    \\nprint_data_info(X_train, X_valid, X_test, y_train, y_valid)'"
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'''def print_data_info(train_data, valid_data, test_data, src_field, trg_field):\n",
    "    \"\"\" This prints some useful stuff about our data sets. \"\"\"\n",
    "\n",
    "    print(\"Data set sizes (number of sentence pairs):\")\n",
    "    print('train', len(train_data))\n",
    "    print('valid', len(valid_data))\n",
    "    print('test', len(test_data), \"\\n\")\n",
    "\n",
    "    print(\"First training example:\")\n",
    "    print(\"src:\", \" \".join(vars(train_data[0])['samples']))\n",
    "    #print(\"trg:\", \" \".join(vars(src_field[0])[1]), \"\\n\")\n",
    "\n",
    "    #print(\"Most common words (src):\")\n",
    "    #print(\"\\n\".join([\"%10s %10d\" % x for x in src_field.vocab.freqs.most_common(10)]), \"\\n\")\n",
    "    #print(\"Most common words (trg):\")\n",
    "    #print(\"\\n\".join([\"%10s %10d\" % x for x in trg_field.vocab.freqs.most_common(10)]), \"\\n\")\n",
    "\n",
    "    #print(\"First 10 words (src):\")\n",
    "    #print(\"\\n\".join(\n",
    "    #    '%02d %s' % (i, t) for i, t in enumerate(src_field.vocab.itos[:10])), \"\\n\")\n",
    "    #print(\"First 10 words (trg):\")\n",
    "    #print(\"\\n\".join(\n",
    "    #    '%02d %s' % (i, t) for i, t in enumerate(trg_field.vocab.itos[:10])), \"\\n\")\n",
    "\n",
    "    print(\"Number of German words (types):\", len(src_field.vocab))\n",
    "    print(\"Number of English words (types):\", len(trg_field.vocab), \"\\n\")\n",
    "    \n",
    "    \n",
    "print_data_info(X_train, X_valid, X_test, y_train, y_valid)'''\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 64\n",
    "trainloader = torch.utils.data.DataLoader(X_train, batch_size=batch_size,\n",
    "                                          shuffle=True, num_workers=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(model, num_epochs=10, lr=0.0003, print_every=100):\n",
    "    print(\"KEK\")\n",
    "\n",
    "    criterion = nn.NLLLoss(reduction=\"sum\", ignore_index=PAD_INDEX)\n",
    "    optim = torch.optim.Adam(model.parameters(), lr=lr)\n",
    "    print(\"KEK\")\n",
    "    for epoch in num_epochs:\n",
    "        running_loss = 0.0\n",
    "        for i, data in enumerate(trainloader, 0):\n",
    "            inputs, labels = data\n",
    "            \n",
    "            optimizer.zero_grad()\n",
    "\n",
    "            outputs = net(inputs)\n",
    "            loss = criterion(outputs, labels)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "            running_loss += loss.item()\n",
    "            if i % 2000 == 1999:    # print every 2000 mini-batches\n",
    "                print('[%d, %5d] loss: %.3f' %\n",
    "                    (epoch + 1, i + 1, running_loss / 2000))\n",
    "                running_loss = 0.0\n",
    "            \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>samples</th>\n      <th>predicted</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>Әлеуметтік желілерде ЕЦ-166/5 төтенше қауіпсіз...</td>\n      <td>Әлеуметтік желілерде ЕЦ жүз алпыс алты бөлу бе...</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>Бұл – фейк, оны Нұр-Сұлтан қаласы бойынша ҚАЖД...</td>\n      <td>Бұл   фейк, оны Нұр Сұлтан қаласы бойынша ҚАЖД...</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>Кейбір желі қолданушылары мекеменің алдына қой...</td>\n      <td>Кейбір желі қолданушылары мекеменің алдына қой...</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>Шын мәнінде дәрігерлер колонияға созылмалы аур...</td>\n      <td>Шын мәнінде дәрігерлер колонияға созылмалы аур...</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>«17 наурызда таңғы сағат 06:30 шамасында Нұр-С...</td>\n      <td>«он жетінші наурызда таңғы сағат алты отыз шам...</td>\n    </tr>\n    <tr>\n      <th>...</th>\n      <td>...</td>\n      <td>...</td>\n    </tr>\n    <tr>\n      <th>3752222</th>\n      <td>Қазақтар шекара шебіндегі бекіністерге жүйелі ...</td>\n      <td>Қазақтар шекара шебіндегі бекіністерге жүйелі ...</td>\n    </tr>\n    <tr>\n      <th>3752223</th>\n      <td>Көтеріліске тағы да Жоламан батыр басшылық етт...</td>\n      <td>Көтеріліске тағы да Жоламан батыр басшылық етт...</td>\n    </tr>\n    <tr>\n      <th>3752224</th>\n      <td>Жоламан батыр бастаған қозғалыстың отаршылдыққ...</td>\n      <td>Жоламан батыр бастаған қозғалыстың отаршылдыққ...</td>\n    </tr>\n    <tr>\n      <th>3752225</th>\n      <td>Көтерілістің әр жерде шашыраңқы жүруі мен оған...</td>\n      <td>Көтерілістің әр жерде шашыраңқы жүруі мен оған...</td>\n    </tr>\n    <tr>\n      <th>3752226</th>\n      <td>Солай бола тұрса да ол Кенесары Қасымұлы баста...</td>\n      <td>Солай бола тұрса да ол Кенесары Қасымұлы баста...</td>\n    </tr>\n  </tbody>\n</table>\n<p>3752227 rows × 2 columns</p>\n</div>",
      "text/plain": "                                                   samples  \\\n0        Әлеуметтік желілерде ЕЦ-166/5 төтенше қауіпсіз...   \n1        Бұл – фейк, оны Нұр-Сұлтан қаласы бойынша ҚАЖД...   \n2        Кейбір желі қолданушылары мекеменің алдына қой...   \n3        Шын мәнінде дәрігерлер колонияға созылмалы аур...   \n4        «17 наурызда таңғы сағат 06:30 шамасында Нұр-С...   \n...                                                    ...   \n3752222  Қазақтар шекара шебіндегі бекіністерге жүйелі ...   \n3752223  Көтеріліске тағы да Жоламан батыр басшылық етт...   \n3752224  Жоламан батыр бастаған қозғалыстың отаршылдыққ...   \n3752225  Көтерілістің әр жерде шашыраңқы жүруі мен оған...   \n3752226  Солай бола тұрса да ол Кенесары Қасымұлы баста...   \n\n                                                 predicted  \n0        Әлеуметтік желілерде ЕЦ жүз алпыс алты бөлу бе...  \n1        Бұл   фейк, оны Нұр Сұлтан қаласы бойынша ҚАЖД...  \n2        Кейбір желі қолданушылары мекеменің алдына қой...  \n3        Шын мәнінде дәрігерлер колонияға созылмалы аур...  \n4        «он жетінші наурызда таңғы сағат алты отыз шам...  \n...                                                    ...  \n3752222  Қазақтар шекара шебіндегі бекіністерге жүйелі ...  \n3752223  Көтеріліске тағы да Жоламан батыр басшылық етт...  \n3752224  Жоламан батыр бастаған қозғалыстың отаршылдыққ...  \n3752225  Көтерілістің әр жерде шашыраңқы жүруі мен оған...  \n3752226  Солай бола тұрса да ол Кенесары Қасымұлы баста...  \n\n[3752227 rows x 2 columns]"
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ghost/anaconda3/lib/python3.8/site-packages/torch/nn/modules/rnn.py:62: UserWarning: dropout option adds dropout after all but last recurrent layer, so non-zero dropout expects num_layers greater than 1, but got dropout=0.2 and num_layers=1\n",
      "  warnings.warn(\"dropout option adds dropout after all but last \"\n"
     ]
    }
   ],
   "source": [
    "model = make_model(len(df['samples']), len(df['predicted']),\n",
    "                   emb_size=256, hidden_size=256,\n",
    "                   num_layers=1, dropout=0.2)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "ename": "Error",
     "evalue": "Session cannot generate requests",
     "output_type": "error",
     "traceback": [
      "Error: Session cannot generate requests",
      "at w.executeCodeCell (/home/ghost/.vscode/extensions/ms-toolsai.jupyter-2021.6.832593372/out/client/extension.js:90:316184)",
      "at w.execute (/home/ghost/.vscode/extensions/ms-toolsai.jupyter-2021.6.832593372/out/client/extension.js:90:315573)",
      "at w.start (/home/ghost/.vscode/extensions/ms-toolsai.jupyter-2021.6.832593372/out/client/extension.js:90:311378)",
      "at runMicrotasks (<anonymous>)",
      "at processTicksAndRejections (internal/process/task_queues.js:93:5)",
      "at async t.CellExecutionQueue.executeQueuedCells (/home/ghost/.vscode/extensions/ms-toolsai.jupyter-2021.6.832593372/out/client/extension.js:90:325786)",
      "at async t.CellExecutionQueue.start (/home/ghost/.vscode/extensions/ms-toolsai.jupyter-2021.6.832593372/out/client/extension.js:90:325326)"
     ]
    }
   ],
   "source": [
    "dev_perplexities = train(model, print_every=100)                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                   \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>samples</th>\n      <th>predicted</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>Әлеуметтік желілерде ЕЦ-166/5 төтенше қауіпсіз...</td>\n      <td>Әлеуметтік желілерде ЕЦ жүз алпыс алты бөлу бе...</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>Бұл – фейк, оны Нұр-Сұлтан қаласы бойынша ҚАЖД...</td>\n      <td>Бұл   фейк, оны Нұр Сұлтан қаласы бойынша ҚАЖД...</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>Кейбір желі қолданушылары мекеменің алдына қой...</td>\n      <td>Кейбір желі қолданушылары мекеменің алдына қой...</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>Шын мәнінде дәрігерлер колонияға созылмалы аур...</td>\n      <td>Шын мәнінде дәрігерлер колонияға созылмалы аур...</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>«17 наурызда таңғы сағат 06:30 шамасында Нұр-С...</td>\n      <td>«он жетінші наурызда таңғы сағат алты отыз шам...</td>\n    </tr>\n    <tr>\n      <th>...</th>\n      <td>...</td>\n      <td>...</td>\n    </tr>\n    <tr>\n      <th>3752222</th>\n      <td>Қазақтар шекара шебіндегі бекіністерге жүйелі ...</td>\n      <td>Қазақтар шекара шебіндегі бекіністерге жүйелі ...</td>\n    </tr>\n    <tr>\n      <th>3752223</th>\n      <td>Көтеріліске тағы да Жоламан батыр басшылық етт...</td>\n      <td>Көтеріліске тағы да Жоламан батыр басшылық етт...</td>\n    </tr>\n    <tr>\n      <th>3752224</th>\n      <td>Жоламан батыр бастаған қозғалыстың отаршылдыққ...</td>\n      <td>Жоламан батыр бастаған қозғалыстың отаршылдыққ...</td>\n    </tr>\n    <tr>\n      <th>3752225</th>\n      <td>Көтерілістің әр жерде шашыраңқы жүруі мен оған...</td>\n      <td>Көтерілістің әр жерде шашыраңқы жүруі мен оған...</td>\n    </tr>\n    <tr>\n      <th>3752226</th>\n      <td>Солай бола тұрса да ол Кенесары Қасымұлы баста...</td>\n      <td>Солай бола тұрса да ол Кенесары Қасымұлы баста...</td>\n    </tr>\n  </tbody>\n</table>\n<p>3752227 rows × 2 columns</p>\n</div>",
      "text/plain": "                                                   samples  \\\n0        Әлеуметтік желілерде ЕЦ-166/5 төтенше қауіпсіз...   \n1        Бұл – фейк, оны Нұр-Сұлтан қаласы бойынша ҚАЖД...   \n2        Кейбір желі қолданушылары мекеменің алдына қой...   \n3        Шын мәнінде дәрігерлер колонияға созылмалы аур...   \n4        «17 наурызда таңғы сағат 06:30 шамасында Нұр-С...   \n...                                                    ...   \n3752222  Қазақтар шекара шебіндегі бекіністерге жүйелі ...   \n3752223  Көтеріліске тағы да Жоламан батыр басшылық етт...   \n3752224  Жоламан батыр бастаған қозғалыстың отаршылдыққ...   \n3752225  Көтерілістің әр жерде шашыраңқы жүруі мен оған...   \n3752226  Солай бола тұрса да ол Кенесары Қасымұлы баста...   \n\n                                                 predicted  \n0        Әлеуметтік желілерде ЕЦ жүз алпыс алты бөлу бе...  \n1        Бұл   фейк, оны Нұр Сұлтан қаласы бойынша ҚАЖД...  \n2        Кейбір желі қолданушылары мекеменің алдына қой...  \n3        Шын мәнінде дәрігерлер колонияға созылмалы аур...  \n4        «он жетінші наурызда таңғы сағат алты отыз шам...  \n...                                                    ...  \n3752222  Қазақтар шекара шебіндегі бекіністерге жүйелі ...  \n3752223  Көтеріліске тағы да Жоламан батыр басшылық етт...  \n3752224  Жоламан батыр бастаған қозғалыстың отаршылдыққ...  \n3752225  Көтерілістің әр жерде шашыраңқы жүруі мен оған...  \n3752226  Солай бола тұрса да ол Кенесары Қасымұлы баста...  \n\n[3752227 rows x 2 columns]"
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torchtext.legacy.data import Field\n",
    "from kaznlp.tokenization.tokhmm import TokenizerHMM\n",
    "from torchtext.legacy.datasets import IWSLT\n",
    "\n",
    "if True:\n",
    "    mdl = os.path.join('kaznlp', 'tokenization', 'tokhmm.mdl')\n",
    "    ininormer = TokenizerHMM(model = mdl)\n",
    "\n",
    "    def tokenize_kz_unnormalized(text):\n",
    "        return [tok.text for tok in ininormer.tokenize(text)]\n",
    "\n",
    "    def tokenize_kz_normalized(text):\n",
    "        return [tok.text for tok in ininormer.tokenize(text)]\n",
    "\n",
    "    UNK_TOKEN = \"<unk>\"\n",
    "    PAD_TOKEN = \"<pad>\"    \n",
    "    SOS_TOKEN = \"<s>\"\n",
    "    EOS_TOKEN = \"</s>\"\n",
    "    LOWER = True\n",
    "    \n",
    "    # we include lengths to provide to the RNNs\n",
    "    SRC = Field(tokenize=tokenize_kz_unnormalized, \n",
    "                     batch_first=True, lower=LOWER, include_lengths=True,\n",
    "                     unk_token=UNK_TOKEN, pad_token=PAD_TOKEN, init_token=None, eos_token=EOS_TOKEN)\n",
    "    TRG = Field(tokenize=tokenize_kz_normalized, \n",
    "                     batch_first=True, lower=LOWER, include_lengths=True,\n",
    "                     unk_token=UNK_TOKEN, pad_token=PAD_TOKEN, init_token=SOS_TOKEN, eos_token=EOS_TOKEN)\n",
    "\n",
    "    MAX_LEN = 25  # NOTE: we filter out a lot of sentences for speed\n",
    "    train_data, valid_data, test_data = IWSLT.splits(\n",
    "        exts=('.samples', '.predicted'), fields=(SRC, TRG), \n",
    "        filter_pred=lambda x: len(vars(x)['src']) <= MAX_LEN and \n",
    "            len(vars(x)['trg']) <= MAX_LEN)\n",
    "    MIN_FREQ = 5  # NOTE: we limit the vocabulary to frequent words for speed\n",
    "    SRC.build_vocab(train_data.src, min_freq=MIN_FREQ)\n",
    "    TRG.build_vocab(train_data.trg, min_freq=MIN_FREQ)\n",
    "    \n",
    "    PAD_INDEX = TRG.vocab.stoi[PAD_TOKEN]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torchtext.datasets import IMDB\n",
    "\n",
    "train_iter = IMDB(split='train')\n",
    "\n",
    "def tokenize(label, line):\n",
    "    return line.split()\n",
    "\n",
    "tokens = []\n",
    "for label, line in train_iter:\n",
    "    print(\"LABEL\", label)\n",
    "    print(\"LINE\", line)\n",
    "    tokens += tokenize(label, line)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading http://fashion-mnist.s3-website.eu-central-1.amazonaws.com/train-images-idx3-ubyte.gz\n",
      "Downloading http://fashion-mnist.s3-website.eu-central-1.amazonaws.com/train-images-idx3-ubyte.gz to data/FashionMNIST/raw/train-images-idx3-ubyte.gz\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "26422272it [00:08, 3050427.68it/s]                              \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting data/FashionMNIST/raw/train-images-idx3-ubyte.gz to data/FashionMNIST/raw\n",
      "\n",
      "Downloading http://fashion-mnist.s3-website.eu-central-1.amazonaws.com/train-labels-idx1-ubyte.gz\n",
      "Downloading http://fashion-mnist.s3-website.eu-central-1.amazonaws.com/train-labels-idx1-ubyte.gz to data/FashionMNIST/raw/train-labels-idx1-ubyte.gz\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "29696it [00:00, 312815.48it/s]           \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting data/FashionMNIST/raw/train-labels-idx1-ubyte.gz to data/FashionMNIST/raw\n",
      "\n",
      "Downloading http://fashion-mnist.s3-website.eu-central-1.amazonaws.com/t10k-images-idx3-ubyte.gz\n",
      "Downloading http://fashion-mnist.s3-website.eu-central-1.amazonaws.com/t10k-images-idx3-ubyte.gz to data/FashionMNIST/raw/t10k-images-idx3-ubyte.gz\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "4422656it [00:01, 2481844.20it/s]                             \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting data/FashionMNIST/raw/t10k-images-idx3-ubyte.gz to data/FashionMNIST/raw\n",
      "\n",
      "Downloading http://fashion-mnist.s3-website.eu-central-1.amazonaws.com/t10k-labels-idx1-ubyte.gz\n",
      "Downloading http://fashion-mnist.s3-website.eu-central-1.amazonaws.com/t10k-labels-idx1-ubyte.gz to data/FashionMNIST/raw/t10k-labels-idx1-ubyte.gz\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "6144it [00:00, 4563450.29it/s]          \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting data/FashionMNIST/raw/t10k-labels-idx1-ubyte.gz to data/FashionMNIST/raw\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ghost/anaconda3/lib/python3.8/site-packages/torchvision/datasets/mnist.py:498: UserWarning: The given NumPy array is not writeable, and PyTorch does not support non-writeable tensors. This means you can write to the underlying (supposedly non-writeable) NumPy array using the tensor. You may want to copy the array to protect its data or make it writeable before converting it to a tensor. This type of warning will be suppressed for the rest of this program. (Triggered internally at  /pytorch/torch/csrc/utils/tensor_numpy.cpp:180.)\n",
      "  return torch.from_numpy(parsed.astype(m[2], copy=False)).view(*s)\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from torch.utils.data import Dataset\n",
    "from torchvision import datasets\n",
    "from torchvision.transforms import ToTensor\n",
    "import matplotlib.pyplot as pl\n",
    "\n",
    "training_data = datasets.FashionMNIST(\n",
    "    root=\"data\",\n",
    "    train=True,\n",
    "    download=True,\n",
    "    transform=ToTensor()\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'training_data' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-17-b9b0fbdee175>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtraining_data\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m: name 'training_data' is not defined"
     ]
    }
   ],
   "source": [
    "print(training_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": "2"
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(training_data[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "training_data[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>samples</th>\n      <th>predicted</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>Әлеуметтік желілерде ЕЦ-166/5 төтенше қауіпсіз...</td>\n      <td>Әлеуметтік желілерде ЕЦ жүз алпыс алты бөлу бе...</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>Бұл – фейк, оны Нұр-Сұлтан қаласы бойынша ҚАЖД...</td>\n      <td>Бұл   фейк, оны Нұр Сұлтан қаласы бойынша ҚАЖД...</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>Кейбір желі қолданушылары мекеменің алдына қой...</td>\n      <td>Кейбір желі қолданушылары мекеменің алдына қой...</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>Шын мәнінде дәрігерлер колонияға созылмалы аур...</td>\n      <td>Шын мәнінде дәрігерлер колонияға созылмалы аур...</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>«17 наурызда таңғы сағат 06:30 шамасында Нұр-С...</td>\n      <td>«он жетінші наурызда таңғы сағат алты отыз шам...</td>\n    </tr>\n    <tr>\n      <th>...</th>\n      <td>...</td>\n      <td>...</td>\n    </tr>\n    <tr>\n      <th>3752222</th>\n      <td>Қазақтар шекара шебіндегі бекіністерге жүйелі ...</td>\n      <td>Қазақтар шекара шебіндегі бекіністерге жүйелі ...</td>\n    </tr>\n    <tr>\n      <th>3752223</th>\n      <td>Көтеріліске тағы да Жоламан батыр басшылық етт...</td>\n      <td>Көтеріліске тағы да Жоламан батыр басшылық етт...</td>\n    </tr>\n    <tr>\n      <th>3752224</th>\n      <td>Жоламан батыр бастаған қозғалыстың отаршылдыққ...</td>\n      <td>Жоламан батыр бастаған қозғалыстың отаршылдыққ...</td>\n    </tr>\n    <tr>\n      <th>3752225</th>\n      <td>Көтерілістің әр жерде шашыраңқы жүруі мен оған...</td>\n      <td>Көтерілістің әр жерде шашыраңқы жүруі мен оған...</td>\n    </tr>\n    <tr>\n      <th>3752226</th>\n      <td>Солай бола тұрса да ол Кенесары Қасымұлы баста...</td>\n      <td>Солай бола тұрса да ол Кенесары Қасымұлы баста...</td>\n    </tr>\n  </tbody>\n</table>\n<p>3752227 rows × 2 columns</p>\n</div>",
      "text/plain": "                                                   samples  \\\n0        Әлеуметтік желілерде ЕЦ-166/5 төтенше қауіпсіз...   \n1        Бұл – фейк, оны Нұр-Сұлтан қаласы бойынша ҚАЖД...   \n2        Кейбір желі қолданушылары мекеменің алдына қой...   \n3        Шын мәнінде дәрігерлер колонияға созылмалы аур...   \n4        «17 наурызда таңғы сағат 06:30 шамасында Нұр-С...   \n...                                                    ...   \n3752222  Қазақтар шекара шебіндегі бекіністерге жүйелі ...   \n3752223  Көтеріліске тағы да Жоламан батыр басшылық етт...   \n3752224  Жоламан батыр бастаған қозғалыстың отаршылдыққ...   \n3752225  Көтерілістің әр жерде шашыраңқы жүруі мен оған...   \n3752226  Солай бола тұрса да ол Кенесары Қасымұлы баста...   \n\n                                                 predicted  \n0        Әлеуметтік желілерде ЕЦ жүз алпыс алты бөлу бе...  \n1        Бұл   фейк, оны Нұр Сұлтан қаласы бойынша ҚАЖД...  \n2        Кейбір желі қолданушылары мекеменің алдына қой...  \n3        Шын мәнінде дәрігерлер колонияға созылмалы аур...  \n4        «он жетінші наурызда таңғы сағат алты отыз шам...  \n...                                                    ...  \n3752222  Қазақтар шекара шебіндегі бекіністерге жүйелі ...  \n3752223  Көтеріліске тағы да Жоламан батыр басшылық етт...  \n3752224  Жоламан батыр бастаған қозғалыстың отаршылдыққ...  \n3752225  Көтерілістің әр жерде шашыраңқы жүруі мен оған...  \n3752226  Солай бола тұрса да ол Кенесары Қасымұлы баста...  \n\n[3752227 rows x 2 columns]"
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "out_test_X.csv\tout_test.zip\t out_train_Y.zip  out_valid_Y.csv\n",
      "out_test_X.zip\tout_train_X.csv  out_train.zip\t  out_valid_Y.zip\n",
      "out_test_Y.csv\tout_train_X.zip  out_valid_X.csv  out_valid.zip\n",
      "out_test_Y.zip\tout_train_Y.csv  out_valid_X.zip\n"
     ]
    }
   ],
   "source": [
    "!ls splitted_csv/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "\n",
    "X_train = pd.read_csv('X_train.txt', error_bad_lines=False, sep='\\n', names=['SRC'])\n",
    "y_train = pd.read_csv('Y_train.txt', error_bad_lines=False, sep='\\n', names=['TRG'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "training_data = X_train['SRC']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "training_data = training_data.values.tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "training_data = {i : training_data[i] for i in range(0, len(training_data))}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "training_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torchtext\n",
    "import torch\n",
    "from torchtext.data.utils import get_tokenizer\n",
    "from collections import Counter\n",
    "from torchtext.vocab import Vocab\n",
    "from torchtext.utils import download_from_url, extract_archive\n",
    "import io\n",
    "\n",
    "def build_vocab(filepath, tokenizer):\n",
    "  counter = Counter()\n",
    "  with io.open(filepath, encoding=\"utf8\") as f:\n",
    "    print(f)\n",
    "    for string_ in f:\n",
    "      print(string_)\n",
    "      counter.update(tokenizer(string_))\n",
    "  return Vocab(counter, specials=['<unk>', '<pad>', '<bos>', '<eos>'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "def data_process(filepaths):\n",
    "  raw_de_iter = iter(io.open(filepaths[0], encoding=\"utf8\"))\n",
    "  raw_en_iter = iter(io.open(filepaths[1], encoding=\"utf8\"))\n",
    "  data = []\n",
    "  for (raw_de, raw_en) in zip(raw_de_iter, raw_en_iter):\n",
    "    de_tensor_ = torch.tensor([de_vocab[token] for token in de_tokenizer(raw_de)],\n",
    "                            dtype=torch.long)\n",
    "    en_tensor_ = torch.tensor([en_vocab[token] for token in en_tokenizer(raw_en)],\n",
    "                            dtype=torch.long)\n",
    "    data.append((de_tensor_, en_tensor_))\n",
    "  return data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ghost/anaconda3/lib/python3.8/site-packages/torchtext/data/utils.py:123: UserWarning: Spacy model \"en\" could not be loaded, trying \"en_core_web_sm\" instead\n",
      "  warnings.warn(f'Spacy model \"{language}\" could not be loaded, trying \"{OLD_MODEL_SHORTCUTS[language]}\" instead')\n"
     ]
    }
   ],
   "source": [
    "from torchtext.vocab import Vocab\n",
    "from torchtext.vocab import build_vocab_from_iterator\n",
    "\n",
    "en_tokenizer = get_tokenizer('spacy', language='en')\n",
    "\n",
    "\n",
    "def build_vocab(filepath, tokenizer):\n",
    "  counter = Counter()\n",
    "  f =  \"I'm big molodec!\"\n",
    "  for string_ in f:\n",
    "    counter.update(tokenizer(string_))\n",
    "  return build_vocab_from_iterator(counter, specials=['<unk>', '<pad>', '<bos>', '<eos>'])\n",
    "\n",
    "\n",
    "en_vocab = build_vocab('kek', en_tokenizer)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": "Vocab()"
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "en_vocab"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": "['Toronto',\n 'konofest',\n 'tarantino',\n '\"',\n 'Koymalik',\n '\"',\n 'filmin',\n ',',\n 'al',\n 'Rodriges',\n 'bolsa',\n 'ozinin',\n '(',\n 'El',\n 'marachi',\n ')',\n 'kinocin',\n 'korcetkemen',\n '.']"
     },
     "execution_count": 61,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "en_tokenizer('Toronto konofest tarantino \"Koymalik\" filmin, al Rodriges bolsa ozinin (El marachi) kinocin korcetkemen.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": "['Торонто',\n 'кинофестивалінде',\n 'Тарантино',\n '«',\n 'Қоймалық',\n 'иттер',\n '»',\n 'фильмін',\n ',',\n 'ал',\n 'Родригес',\n 'болса',\n 'өзінің',\n '«',\n 'Эль',\n 'Мариачи',\n '»',\n '(',\n 'El',\n 'Mariachi',\n ')',\n 'киносын',\n 'көрсеткен',\n '.']"
     },
     "execution_count": 60,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def flatten(t):\n",
    "    return [item for sublist in t for item in sublist]\n",
    "\n",
    "out = ininormer.tokenize('Торонто кинофестивалінде Тарантино «Қоймалық иттер» фильмін, ал Родригес болса өзінің «Эль Мариачи» (El Mariachi) киносын көрсеткен.')\n",
    "out = flatten(out)\n",
    "out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "__init__() got an unexpected keyword argument 'specials'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-70-b97ba10f2578>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     19\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     20\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 21\u001b[0;31m \u001b[0munnormalize_text\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mbuild_vocab\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'splitted_csv/out_test_X.txt'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtokenize_kz_unnormalized\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     22\u001b[0m \u001b[0;31m#normalize_text = build_vocab('splitted_csv/out_train_Y.csv', tokenize_kz_normalized)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-70-b97ba10f2578>\u001b[0m in \u001b[0;36mbuild_vocab\u001b[0;34m(filepath, tokenizer)\u001b[0m\n\u001b[1;32m      4\u001b[0m     \u001b[0;32mfor\u001b[0m \u001b[0mstring_\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m       \u001b[0mcounter\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mupdate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtokenizer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstring_\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 6\u001b[0;31m   \u001b[0;32mreturn\u001b[0m \u001b[0mVocab\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcounter\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mspecials\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'<unk>'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'<pad>'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'<bos>'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'<eos>'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      7\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      8\u001b[0m \u001b[0mininormer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mTokenizerHMM\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmdl\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mTypeError\u001b[0m: __init__() got an unexpected keyword argument 'specials'"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "def build_vocab(filepath, tokenizer):\n",
    "  counter = Counter()\n",
    "  with io.open(filepath, encoding=\"utf8\") as f:\n",
    "    for string_ in f:\n",
    "      counter.update(tokenizer(string_))\n",
    "  return build_vocab_from_iterator(counter, specials=['<unk>', '<pad>', '<bos>', '<eos>'])\n",
    "\n",
    "ininormer = TokenizerHMM(model = mdl)\n",
    "\n",
    "def tokenize_kz_unnormalized(text):\n",
    "    out = [tok for tok in ininormer.tokenize(text)]\n",
    "    out = flatten(out)\n",
    "    return out\n",
    "\n",
    "def tokenize_kz_normalized(text):\n",
    "    out = [tok for tok in ininormer.tokenize(text)]\n",
    "    out = flatten(out)\n",
    "    return out\n",
    "\n",
    "\n",
    "unnormalize_text = build_vocab('splitted_csv/out_test_X.txt', tokenize_kz_unnormalized)\n",
    "#normalize_text = build_vocab('splitted_csv/out_train_Y.csv', tokenize_kz_normalized)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "normalize_text = build_vocab('splitted_csv/out_train_Y.csv', tokenize_kz_normalized)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'df' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-34-00cf07b74dcd>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mdf\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m: name 'df' is not defined"
     ]
    }
   ],
   "source": [
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.8.5 64-bit ('base': conda)",
   "name": "python385jvsc74a57bd009370de619b8b9149cc7c753872ea6aa5871e135f833321ec46ee8dac5b2b335"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  },
  "metadata": {
   "interpreter": {
    "hash": "809ecebc32aa3bf58f03e9c8fa59f2b8da1bd338679f330b35491bef5973dc5e"
   }
  },
  "orig_nbformat": 2
 },
 "nbformat": 4,
 "nbformat_minor": 2
}